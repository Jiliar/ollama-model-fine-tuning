{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23c4f31-227b-4f9a-98d4-ba38f22ec8e1",
   "metadata": {},
   "source": [
    "### Carga del Modelo Base\n",
    "Vamos a traer a Qwen2.5-Coder-14B a la memoria de tu RTX 3090, comprimido en 4-bits para que entre sin problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9cbf0e7-39e5-4609-b165-6ffc520b9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       # Permite interactuar con el sistema operativo (rutas de carpetas, variables de entorno).\n",
    "import gc       # \"Garbage Collector\": Se usa para liberar memoria RAM/VRAM manualmente si es necesario.\n",
    "import json     # Para manipular archivos JSON (lectura de datasets o configuraciones).\n",
    "import torch    # La librer√≠a principal de PyTorch para operaciones con tensores y uso de la GPU.\n",
    "import shutil   # √ötil para operaciones de archivos de alto nivel, como borrar o mover carpetas completas.\n",
    "import subprocess # Permite ejecutar comandos de terminal (como git o pip) desde Python.\n",
    "\n",
    "# Importa la clase Dataset de Hugging Face para estructurar los datos de entrenamiento.\n",
    "from datasets import Dataset \n",
    "\n",
    "# Unsloth: Librer√≠a optimizada para entrenar modelos m√°s r√°pido y con menos memoria.\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "# Permite obtener informaci√≥n t√©cnica de un modelo alojado en el Hugging Face Hub.\n",
    "from huggingface_hub import model_info\n",
    "\n",
    "# Facilita la aplicaci√≥n de formatos de chat (como Llama-3 o Alpaca) a los datos.\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# El \"Entrenador\" (Trainer) especializado en Supervised Fine-Tuning (SFT).\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Define los hiperpar√°metros del entrenamiento (√©pocas, tasa de aprendizaje, pasos, etc.).\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158f1dca-a9c0-4393-9aff-e9dfd1754beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo encontrado en cach√©: /root/.cache/huggingface/hub/models--unsloth--Qwen2.5-Coder-14B-Instruct-bnb-4bit\n",
      "üîÑ Cargando modelo localmente...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 22.152 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:55<00:00, 27.51s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo cargado exitosamente en 4-bits.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuraci√≥n de par√°metros iniciales ---\n",
    "\n",
    "# Define la longitud m√°xima de tokens (contexto) que el modelo procesar√°. \n",
    "# 2048 es est√°ndar, pero Unsloth permite ampliarlo din√°micamente.\n",
    "max_seq_length = 2048 \n",
    "\n",
    "# El tipo de datos para los pesos (None deja que Unsloth lo detecte autom√°ticamente).\n",
    "# Usualmente detectar√° float16 o bfloat16 seg√∫n tu GPU.\n",
    "dtype = None \n",
    "\n",
    "# Activa la cuantizaci√≥n de 4 bits. Crucial para que un modelo de 14B \n",
    "# quepa en GPUs de consumo (como una RTX 3060/4060 o superiores).\n",
    "load_in_4bit = True \n",
    "\n",
    "# El identificador del modelo en Hugging Face. \n",
    "# Esta versi√≥n ya viene pre-cuantizada (\"bnb-4bit\") para ser ultra r√°pida.\n",
    "model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\"\n",
    "\n",
    "# --- Verificaci√≥n de archivos locales (Cach√©) ---\n",
    "\n",
    "# Construye la ruta donde Hugging Face suele guardar los modelos descargados.\n",
    "# Transforma \"usuario/modelo\" en el formato de carpetas de cach√© del sistema.\n",
    "cache_dir = os.path.expanduser(f\"~/.cache/huggingface/hub/models--{'--'.join(model_name.split('/'))}\")\n",
    "\n",
    "# Comprueba si la carpeta del modelo ya existe en el disco duro.\n",
    "if os.path.exists(cache_dir):\n",
    "    # Si existe, nos avisa que no gastar√° internet descarg√°ndolo de nuevo.\n",
    "    print(f\"‚úÖ Modelo encontrado en cach√©: {cache_dir}\")\n",
    "    print(\"üîÑ Cargando modelo localmente...\")\n",
    "else:\n",
    "    # Si no existe, nos advierte que iniciar√° una descarga pesada.\n",
    "    print(f\"‚ö†Ô∏è Modelo NO encontrado en cach√©. Se descargar√° en: {cache_dir}\")\n",
    "\n",
    "# --- Carga del Modelo y el Tokenizador ---\n",
    "\n",
    "# Utiliza la funci√≥n optimizada de Unsloth para cargar el modelo en la VRAM de la GPU.\n",
    "# Retorna dos objetos: \n",
    "# 1. model: El cerebro del IA.\n",
    "# 2. tokenizer: El traductor que convierte texto en n√∫meros (tokens).\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# Confirmaci√≥n final de que el modelo est√° listo para usarse o entrenarse.\n",
    "print(\"‚úÖ Modelo cargado exitosamente en 4-bits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c773f-4058-4538-9ff8-d6ae84ee1a79",
   "metadata": {},
   "source": [
    "### Verificar que el modelo existe manualmente\n",
    "Si ninguna de las anteriores funciona, verifica que puedes acceder al modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0710312-e904-44c1-864b-b511e399badf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo encontrado: unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\n",
      "üì• Descargas: 8706\n"
     ]
    }
   ],
   "source": [
    "# --- Verificaci√≥n de metadatos en Hugging Face Hub ---\n",
    "\n",
    "# Usamos un bloque \"try-except\" para manejar posibles errores de conexi√≥n o permisos.\n",
    "try:\n",
    "    # Llama a la API de Hugging Face para obtener la ficha t√©cnica (info) del modelo.\n",
    "    # Esto no descarga el modelo, solo consulta sus estad√≠sticas y estado.\n",
    "    info = model_info(\"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\")\n",
    "    \n",
    "    # Si la consulta es exitosa, imprime el ID confirmado del modelo.\n",
    "    print(f\"‚úÖ Modelo encontrado: {info.id}\")\n",
    "    \n",
    "    # Muestra el n√∫mero total de descargas que ha tenido el modelo (popularidad).\n",
    "    print(f\"üì• Descargas: {info.downloads}\")\n",
    "\n",
    "# Si ocurre un error (ej. no hay internet, el modelo es privado o el nombre est√° mal escrito):\n",
    "except Exception as e:\n",
    "    # Atrapa el error y lo muestra en pantalla sin detener la ejecuci√≥n del programa.\n",
    "    print(f\"‚ùå Error accediendo al modelo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827c010-58fb-427a-8181-32708028502e",
   "metadata": {},
   "source": [
    "### Inyecci√≥n de Adaptadores (LoRA)\n",
    "Aqu√≠ definimos la arquitectura del fine-tuning. Solo vamos a entrenar una fracci√≥n del modelo (los adaptadores), lo que hace que el proceso sea r√°pido y eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff301f5-a741-40d4-a4b5-fa806bbf9150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.2.1 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Adaptadores LoRA configurados.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuraci√≥n de PEFT (Parameter-Efficient Fine-Tuning) con LoRA ---\n",
    "\n",
    "# Transforma el modelo base en un modelo PEFT (solo una parte es entrenable).\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    # 'r' (Rank): Define el tama√±o de las matrices de bajo rango. \n",
    "    # 16 es un equilibrio ideal entre precisi√≥n y ahorro de memoria.\n",
    "    r = 16, \n",
    "\n",
    "    # 'target_modules': Especifica en qu√© capas del modelo se inyectar√°n los adaptadores.\n",
    "    # Estas capas (q, k, v, o, gate, up, down) cubren casi toda la atenci√≥n y redes neuronales del modelo.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "\n",
    "    # 'lora_alpha': Escala el aprendizaje de los adaptadores. \n",
    "    # Generalmente se recomienda que sea igual o el doble de 'r'.\n",
    "    lora_alpha = 16,\n",
    "\n",
    "    # 'lora_dropout': Probabilidad de desactivar neuronas al azar para evitar sobreajuste.\n",
    "    # 0 es lo m√°s eficiente para velocidad de entrenamiento en Unsloth.\n",
    "    lora_dropout = 0, \n",
    "\n",
    "    # 'bias': Define si se entrenan los sesgos. \"none\" es lo est√°ndar para LoRA.\n",
    "    bias = \"none\",\n",
    "\n",
    "    # 'use_gradient_checkpointing': T√©cnica que libera memoria RAM de la GPU \n",
    "    # guardando solo lo esencial. \"unsloth\" usa una versi√≥n optimizada que gasta un 30% menos.\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "\n",
    "    # Semilla aleatoria para que los resultados sean reproducibles (siempre den lo mismo).\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# Mensaje de confirmaci√≥n: el modelo ahora est√° listo para recibir datos de entrenamiento.\n",
    "print(\"‚úÖ Adaptadores LoRA configurados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c7aeb-6101-4a9a-87e1-61c9692e44ed",
   "metadata": {},
   "source": [
    "#### Preparaci√≥n del Dataset\n",
    "Le ense√±amos al modelo a entender el formato de tu dataset.jsonl (ChatML: System, User, Assistant). Asumimos que dentro de tu JSONL, el arreglo de mensajes se llama messages o conversations.\n",
    "\n",
    "Este c√≥digo es fundamental: act√∫a como el traductor entre la forma en que t√∫ guardaste la informaci√≥n en tu archivo `epics.jsonl` y la forma exacta en la que el modelo (Qwen2.5) necesita leerla para aprender.\n",
    "\n",
    "En t√©rminos sencillos, el modelo no entiende \"columnas\" o \"diccionarios\"; solo entiende secuencias largas de texto con etiquetas especiales que le indican qui√©n est√° hablando.\n",
    "\n",
    "#### 1. Aplicar la plantilla ChatML (`get_chat_template`)\n",
    "\n",
    "Los modelos conversacionales como Qwen usan un formato llamado **ChatML** (Chat Markup Language). Esto significa que usan \"etiquetas invisibles\" para separar los mensajes, como `<|im_start|>user` y `<|im_end|>`.\n",
    "Este paso configura tu tokenizador para que inyecte autom√°ticamente estas etiquetas en el texto, ahorr√°ndote el trabajo de escribirlas a mano.\n",
    "\n",
    "#### 2. Definir el System Prompt\n",
    "\n",
    "Aqu√≠ le inyectamos la personalidad a tu PM Senior. Le estamos diciendo expl√≠citamente cu√°l es su rol y, muy importante, le indicamos que **debe responder en formato JSON**. Esto ancla el comportamiento del modelo para que siempre act√∫e como un experto estructurado.\n",
    "\n",
    "#### 3. La funci√≥n de formateo (`formatting_prompts_func`)\n",
    "\n",
    "Este es el \"motor\" de la celda. Como tu archivo tiene una columna llamada `input` y otra llamada `output`, la funci√≥n hace lo siguiente para cada fila de tu dataset:\n",
    "\n",
    "* **Extrae los datos:** Toma el diccionario crudo de `input` y el de `output`.\n",
    "* **Formatea a texto JSON legible (`json.dumps`):** Este paso es el truco vital. Como tu objetivo es que el modelo devuelva un JSON perfecto (como se ve en tus salidas con `epic_id`, `title`, `acceptance_criteria`), usamos `json.dumps(..., indent=2)` para transformar tus diccionarios en cadenas de texto con saltos de l√≠nea y tabulaciones perfectas. As√≠ el modelo aprende a indentar como un humano.\n",
    "* **Arma la conversaci√≥n:** Crea una lista l√≥gica con tres roles:\n",
    "1. El `system` (las instrucciones base).\n",
    "2. El `user` (tu `input` con el contexto y requerimientos).\n",
    "3. El `assistant` (tu `output` con la Epic dorada).\n",
    "\n",
    "\n",
    "* **Aplica la plantilla (`apply_chat_template`):** Pasa esa lista estructurada por el tokenizador para fusionarla en un √∫nico bloque de texto continuo con todas las etiquetas ChatML listas para el entrenamiento.\n",
    "\n",
    "#### 4. Cargar y procesar (`load_dataset` y `map`)\n",
    "\n",
    "* Usa la librer√≠a de Hugging Face para cargar tu archivo `epics.jsonl` a la memoria RAM de tu RunPod.\n",
    "* El comando `.map(..., batched=True)` pasa todo tu dataset por la funci√≥n que explicamos arriba de forma simult√°nea y s√∫per r√°pida.\n",
    "* El resultado final es que tu dataset ahora tiene una nueva columna llamada **`text`**, que contiene la conversaci√≥n perfectamente formateada. Esta columna `text` es la *√∫nica* que Qwen va a leer durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13f2245-b713-4332-aa75-511e7cce324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°Dataset cargado y formateado! Total de ejemplos procesados: 104\n",
      "\n",
      "--- MUESTRA DEL PRIMER EJEMPLO FORMATEADO ---\n",
      "<|im_start|>system\n",
      "Eres un Product Manager Senior experto. Tu tarea es analizar el contexto y los requerimientos proporcionados para redactar Epics de software detalladas, estructuradas y precisas en formato JSON.<|im_end|>\n",
      "<|im_start|>user\n",
      "{\n",
      "  \"context\": \"El proyecto inicia desde cero, sin ning√∫n tipo de infraestructura en la nube. Es imperativo establecer una base s√≥lida, repetible y segura que permita el despliegue y la operaci√≥n de todos los componentes subsecuentes de la plataforma. La adop...\n",
      "[CONTIN√öA]\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Preparaci√≥n del Formato de Conversaci√≥n ---\n",
    "\n",
    "# Configura el tokenizador para que use la estructura \"ChatML\" (<|im_start|>, <|im_end|>).\n",
    "# Esto es vital para que el modelo sepa cu√°ndo termina de hablar el usuario y empieza √©l.\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\",\n",
    ")\n",
    "\n",
    "# --- 2. Definici√≥n de la Identidad (System Prompt) ---\n",
    "\n",
    "# Establecemos las \"instrucciones de comportamiento\" del modelo. \n",
    "# Aqu√≠ le decimos que sea un Product Manager experto y que responda en JSON.\n",
    "system_prompt = \"Eres un Product Manager Senior experto. Tu tarea es analizar el contexto y los requerimientos proporcionados para redactar Epics de software detalladas, estructuradas y precisas en formato JSON.\"\n",
    "\n",
    "formatted_texts = []\n",
    "\n",
    "# --- 3. Procesamiento Manual del Archivo de Datos ---\n",
    "\n",
    "# Ruta donde tienes guardados tus ejemplos de entrenamiento (formato JSON Lines).\n",
    "file_path = \"../data/epics.jsonl\" \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        # Si la l√≠nea est√° vac√≠a, nos la saltamos para evitar errores.\n",
    "        if not line.strip(): continue \n",
    "        \n",
    "        # Convertimos la l√≠nea de texto (JSON) en un diccionario de Python.\n",
    "        record = json.loads(line)\n",
    "        \n",
    "        # Convertimos los campos 'input' y 'output' en texto (strings) bien formateados.\n",
    "        # 'ensure_ascii=False' permite tildes y √±; 'indent=2' lo hace legible.\n",
    "        user_content = json.dumps(record[\"input\"], ensure_ascii=False, indent=2)\n",
    "        assistant_content = json.dumps(record[\"output\"], ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Creamos la estructura de la conversaci√≥n (Mensaje de Sistema -> Usuario -> Asistente).\n",
    "        convo = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        \n",
    "        # 'apply_chat_template' une todo lo anterior usando las etiquetas especiales de ChatML.\n",
    "        # tokenize=False: Solo genera el texto plano por ahora.\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        \n",
    "        # Guardamos el resultado en una lista de diccionarios con la clave \"text\".\n",
    "        formatted_texts.append({\"text\": text})\n",
    "\n",
    "# --- 4. Creaci√≥n del Dataset Final ---\n",
    "\n",
    "# Convertimos nuestra lista de Python en un objeto Dataset de HuggingFace.\n",
    "# Este formato es el que el 'SFTTrainer' de Unsloth requiere para empezar a entrenar.\n",
    "dataset = Dataset.from_list(formatted_texts)\n",
    "\n",
    "# Mensajes de control para verificar que todo sali√≥ bien.\n",
    "print(f\"‚úÖ ¬°Dataset cargado y formateado! Total de ejemplos procesados: {len(dataset)}\")\n",
    "\n",
    "# Imprimimos los primeros 500 caracteres del primer ejemplo para ver las etiquetas <|im_start|>.\n",
    "print(\"\\n--- MUESTRA DEL PRIMER EJEMPLO FORMATEADO ---\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\\n[CONTIN√öA]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a428ef1-0453-4c79-8a4a-cefd7378a9fe",
   "metadata": {},
   "source": [
    "### **Fine-Tunning** (ajusta) un modelo de lenguaje pre-entrenado con tus datos espec√≠ficos usando **LoRA**\n",
    "\n",
    "| Componente | Funci√≥n |\n",
    "|------------|---------|\n",
    "| `SFTTrainer` | Entrenador especializado para \"Supervised Fine-Tuning\" (ajuste supervisado) |\n",
    "| `train_dataset` | Tus datos de entrenamiento (104 ejemplos conversacionales) |\n",
    "| `max_seq_length` | L√≠mite de tokens por ejemplo (2048) |\n",
    "| `packing=False` | Respeta la estructura conversacional exacta de cada ejemplo |\n",
    "| `per_device_train_batch_size=2` | Procesa 2 ejemplos simult√°neamente en GPU |\n",
    "| `gradient_accumulation_steps=4` | Simula un lote de 8 ejemplos (2√ó4) para ahorrar VRAM |\n",
    "| `num_train_epochs=3` | Pasa 3 veces por todo el dataset (~39 pasos totales) |\n",
    "| `learning_rate=2e-4` | Velocidad de aprendizaje est√°ndar para LoRA |\n",
    "| `adamw_8bit` | Optimizador comprimido que usa menos memoria |\n",
    "| `fp16/bf16` | Precisi√≥n mixta para acelerar entrenamiento |\n",
    "\n",
    "**Tiempo estimado:** ~39 pasos √ó tiempo por paso (var√≠a seg√∫n GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01333232-cdf6-408e-a226-05feefb79916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Cargando modelo base...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 22.152 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Inyectando adaptadores LoRA (Modo Ligero)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2026.2.1 patched 48 layers with 48 QKV layers, 48 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Formateando el dataset...\n",
      "üöÄ ¬°INICIANDO FINE-TUNING EXTREMO!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=64): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104/104 [00:13<00:00,  7.70 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 104 | Num Epochs = 3 | Total steps = 39\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 12,582,912 of 14,782,616,576 (0.09% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 02:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.722500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.813900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.603400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.371400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°Entrenamiento completado en 165.8221 segundos!\n"
     ]
    }
   ],
   "source": [
    "# --- 0. LIMPIEZA PROFUNDA DE GPU ---\n",
    "# Libera cualquier residuo de memoria en la GPU para empezar desde cero.\n",
    "torch.cuda.empty_cache() \n",
    "# Fuerza al recolector de basura de Python a limpiar objetos no utilizados en la RAM.\n",
    "gc.collect() \n",
    "\n",
    "# --- 1. CARGA DEL MODELO ---\n",
    "# Aumentamos la longitud de secuencia a 4096 para manejar Epics m√°s largas.\n",
    "max_seq_length = 4096 \n",
    "print(\"‚è≥ Cargando modelo base...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Cuantizaci√≥n para reducir el peso del modelo en VRAM.\n",
    ")\n",
    "\n",
    "# --- 2. ADAPTADORES LORA (MODO LIGERO) ---\n",
    "print(\"üß† Inyectando adaptadores LoRA (Modo Ligero)...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Reducimos el rango (Rank). Menos par√°metros entrenables = menos memoria.\n",
    "    # Solo entrenamos las capas de atenci√≥n (proyecciones Q, K, V, O).\n",
    "    # Al quitar \"gate_proj\", \"up_proj\" y \"down_proj\", ahorramos mucha VRAM.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# --- 3. PREPARAR DATASET ---\n",
    "# (Este bloque aplica la plantilla ChatML y limpia el JSONL como vimos anteriormente)\n",
    "print(\"üìä Formateando el dataset...\")\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"chatml\")\n",
    "system_prompt = \"Eres un Product Manager Senior experto...\"\n",
    "formatted_texts = []\n",
    "\n",
    "with open(\"../data/epics.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip(): continue\n",
    "        record = json.loads(line)\n",
    "        user_content = json.dumps(record[\"input\"], ensure_ascii=False, indent=2)\n",
    "        assistant_content = json.dumps(record[\"output\"], ensure_ascii=False, indent=2)\n",
    "        convo = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        formatted_texts.append({\"text\": text})\n",
    "\n",
    "dataset = Dataset.from_list(formatted_texts)\n",
    "\n",
    "# --- 4. ENTRENAMIENTO BLINDADO CONTRA OOM ---\n",
    "print(\"üöÄ ¬°INICIANDO FINE-TUNING EXTREMO!\")\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2, # Usa 2 procesos para cargar datos m√°s r√°pido.\n",
    "    packing = False,      # No empaqueta secuencias cortas (m√°s lento pero m√°s estable).\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1, # Procesa 1 ejemplo a la vez para no saturar la GPU.\n",
    "        gradient_accumulation_steps = 8, # Actualiza pesos cada 8 pasos (Batch efectivo = 8).\n",
    "        warmup_steps = 5,                # Sube la intensidad del aprendizaje gradualmente.\n",
    "        num_train_epochs = 3,            # El modelo ver√° el dataset completo 3 veces.\n",
    "        learning_rate = 2e-4,            # Velocidad de aprendizaje (est√°ndar para LoRA).\n",
    "        # Selecciona autom√°ticamente entre fp16 o bf16 seg√∫n la potencia de tu GPU.\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,               # Muestra el progreso en cada paso.\n",
    "        # \"paged_adamw_8bit\": Si la GPU se queda sin memoria, usa la RAM del sistema como \"colch√≥n\".\n",
    "        optim = \"paged_adamw_8bit\", \n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",          # Carpeta donde se guardar√°n los checkpoints.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Inicia el proceso de entrenamiento y guarda las estad√≠sticas finales.\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"‚úÖ ¬°Entrenamiento completado en {trainer_stats.metrics['train_runtime']} segundos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843da0b1-bc9e-4765-9fea-282216b45da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease   \n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Fetched 384 kB in 1s (433 kB/s)                                   \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "build-essential set to manually installed.\n",
      "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
      "libcurl4-openssl-dev is already the newest version (7.81.0-1ubuntu1.21).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 134 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# 'apt-get update -y': Actualiza la lista de paquetes disponibles en los repositorios.\n",
    "# El flag '-y' responde autom√°ticamente \"s√≠\" a las confirmaciones para que no se detenga.\n",
    "!apt-get update -y && \\\n",
    "\n",
    "# 'apt-get install -y': Comando para instalar nuevos paquetes.\n",
    "# Se instalan 3 componentes fundamentales para compilar software:\n",
    "# 1. cmake: Herramienta avanzada para gestionar el proceso de compilaci√≥n (indispensable para muchas librer√≠as de IA).\n",
    "# 2. build-essential: Un paquete que incluye el compilador GCC, G++ y herramientas b√°sicas para crear software desde el c√≥digo fuente.\n",
    "# 3. libcurl4-openssl-dev: Librer√≠a necesaria para que las aplicaciones puedan realizar transferencias de red (como descargar modelos o comunicarse con APIs) de forma segura.\n",
    "apt-get install -y cmake build-essential libcurl4-openssl-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3b5cc1-12e6-4e7d-bd10-919692dbcab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  - \u001b[36mgguf\u001b[39m\n",
      "  - \u001b[36mprotobuf\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Using version \u001b[39;1m^0.2.1\u001b[39;22m for \u001b[36msentencepiece\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(0.6s)\u001b[39;22m\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# '!': Ejecuta el comando en la consola del sistema.\n",
    "# 'poetry add': Instala las librer√≠as y las registra en tu archivo de proyecto.\n",
    "\n",
    "!poetry add \\\n",
    "    # 'gguf': Necesario para escribir y leer el formato de archivo final (.gguf).\n",
    "    gguf \\\n",
    "    \n",
    "    # 'protobuf': El sistema de serializaci√≥n de datos que usa Google y Hugging Face.\n",
    "    protobuf \\\n",
    "    \n",
    "    # 'sentencepiece': ¬°La pieza clave! Es la librer√≠a que maneja la tokenizaci√≥n \n",
    "    # de modelos como Qwen, Llama y Mistral. Sin esto, el modelo no puede \n",
    "    # descomponer las palabras en unidades que entienda (tokens).\n",
    "    sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22105179-cc95-4f07-a87b-7524542e690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '!': Indica que el comando se ejecuta en la consola del sistema (Shell).\n",
    "# 'rm': Es el comando para \"remover\" (borrar) archivos o directorios.\n",
    "\n",
    "!rm -rf \\\n",
    "    # '-r' (recursive): Permite borrar carpetas y todo su contenido interno (subcarpetas y archivos).\n",
    "    # '-f' (force): Fuerza el borrado ignorando archivos inexistentes y sin pedir confirmaci√≥n al usuario.\n",
    "    -rf \\\n",
    "    \n",
    "    # 'llama.cpp': El nombre de la carpeta espec√≠fica que se desea eliminar.\n",
    "    llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "700780ac-532f-44f6-a310-b8d74d04a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ llama.cpp ya existe\n",
      "üì¶ Iniciando fusi√≥n y exportaci√≥n a GGUF...\n",
      "‚è≥ Esto puede tomar 10-20 minutos...\n",
      "‚ö†Ô∏è Error con m√©todo autom√°tico: unsloth_save_pretrained_gguf() got an unexpected keyword argument 'converter_location'\n",
      "üîÑ Intentando m√©todo manual...\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00006.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:28<00:00, 24.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [01:10<00:00, 11.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/code/MiPM_Senior_HF`\n",
      "‚úÖ Modelo guardado en formato HF\n",
      "üìù Para convertir a GGUF manualmente, ejecuta en terminal:\n",
      "\n",
      "    cd ./llama.cpp\n",
      "    python convert_hf_to_gguf.py ../MiPM_Senior_HF --outfile ../MiPM_Senior.gguf --outtype q4_k_m\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# --- Verificaci√≥n y Setup de llama.cpp ---\n",
    "\n",
    "# Definimos la ruta local donde queremos que viva el repositorio llama.cpp.\n",
    "LLAMA_CPP_DIR = \"./llama.cpp\"\n",
    "\n",
    "# Comprobamos si la carpeta ya existe para no descargarla dos veces.\n",
    "if not os.path.exists(LLAMA_CPP_DIR):\n",
    "    print(\"üì• Descargando llama.cpp...\")\n",
    "    # 'git clone': Descarga el c√≥digo fuente oficial de llama.cpp.\n",
    "    subprocess.run([\n",
    "        \"git\", \"clone\", \n",
    "        \"https://github.com/ggerganov/llama.cpp.git\",\n",
    "        LLAMA_CPP_DIR\n",
    "    ], check=True)\n",
    "    print(\"‚úÖ llama.cpp descargado\")\n",
    "    \n",
    "    # Instalamos las librer√≠as de Python necesarias para que los scripts de conversi√≥n funcionen.\n",
    "    print(\"üì¶ Instalando dependencias...\")\n",
    "    subprocess.run([\n",
    "        \"pip\", \"install\", \"-r\", \n",
    "        f\"{LLAMA_CPP_DIR}/requirements.txt\"\n",
    "    ], check=True)\n",
    "else:\n",
    "    # Si la carpeta ya est√° ah√≠, simplemente lo confirmamos.\n",
    "    print(\"‚úÖ llama.cpp ya existe\")\n",
    "\n",
    "# --- Proceso de Exportaci√≥n ---\n",
    "\n",
    "print(\"üì¶ Iniciando fusi√≥n y exportaci√≥n a GGUF...\")\n",
    "print(\"‚è≥ Esto puede tomar 10-20 minutos...\")\n",
    "\n",
    "try:\n",
    "    # Intento A: Usar la funci√≥n integrada de Unsloth.\n",
    "    model.save_pretrained_gguf(\n",
    "        \"MiPM_Senior\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_m\", # Cuantizaci√≥n de 4 bits (calidad media-alta).\n",
    "        # Indicamos expl√≠citamente d√≥nde est√° la herramienta de conversi√≥n.\n",
    "        converter_location=LLAMA_CPP_DIR,\n",
    "    )\n",
    "    print(\"‚úÖ ¬°PROCESO COMPLETADO!\")\n",
    "    print(\"üìÅ Archivo generado: MiPM_Senior_q4_k_m.gguf\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Intento B (Fallback): Si lo anterior falla (por errores de memoria o librer√≠as),\n",
    "    # hacemos el proceso en dos pasos manuales.\n",
    "    print(f\"‚ö†Ô∏è Error con m√©todo autom√°tico: {e}\")\n",
    "    print(\"üîÑ Intentando m√©todo manual...\")\n",
    "    \n",
    "    # 1. Primero, fusionamos el modelo LoRA con el base y lo guardamos como un modelo normal de Hugging Face.\n",
    "    # 'merged_16bit' asegura que no haya p√©rdida de calidad en esta fase.\n",
    "    model.save_pretrained_merged(\n",
    "        \"MiPM_Senior_HF\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\",\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Modelo guardado en formato HF\")\n",
    "    # 2. Instrucciones para que el usuario ejecute la conversi√≥n final desde la consola.\n",
    "    print(\"üìù Para convertir a GGUF manualmente, ejecuta en terminal:\")\n",
    "    print(f\"\"\"\n",
    "    cd {LLAMA_CPP_DIR}\n",
    "    python convert_hf_to_gguf.py ../MiPM_Senior_HF --outfile ../MiPM_Senior.gguf --outtype q4_k_m\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "239a5721-0d86-4640-83fb-25346d729775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease   \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
      "g++ set to manually installed.\n",
      "gcc is already the newest version (4:11.2.0-1ubuntu1).\n",
      "gcc set to manually installed.\n",
      "make is already the newest version (4.3-4.1build1).\n",
      "make set to manually installed.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 134 not upgraded.\n",
      "\n",
      "--- ESPACIO EN DISCO ---\n",
      "Filesystem                    Size  Used Avail Use% Mounted on\n",
      "mfs#eur-no-1.runpod.net:9421 1006T  591T  416T  59% /workspace\n"
     ]
    }
   ],
   "source": [
    "# Instalar herramientas de compilaci√≥n esenciales\n",
    "!apt-get update && apt-get install -y build-essential gcc g++ make\n",
    "\n",
    "# Verificar espacio en disco en /workspace\n",
    "print(\"\\n--- ESPACIO EN DISCO ---\")\n",
    "!df -h /workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60f8fe-aea8-4761-bb57-9479e323b6fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando proceso para: /workspace/code/MiPM_Senior_HF\n",
      "üì¶ Asegurando dependencias en entorno Poetry...\n",
      "Requirement already satisfied: gguf in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (0.17.1)\n",
      "Requirement already satisfied: sentencepiece in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (4.25.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from gguf) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from gguf) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from gguf) (4.67.3)\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\n",
      "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
      "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
      "Requirement already satisfied: numpy~=1.26.4 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: sentencepiece<0.3.0,>=0.1.98 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.57.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (4.57.6)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 6)) (0.17.1)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 7)) (4.25.8)\n",
      "Requirement already satisfied: torch~=2.6.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2.6.0+cpu)\n",
      "Requirement already satisfied: aiohttp~=3.9.3 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1)) (3.9.5)\n",
      "Requirement already satisfied: pytest~=8.3.3 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 2)) (8.3.5)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.34.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 3)) (0.36.2)\n",
      "Requirement already satisfied: matplotlib~=3.10.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (3.10.8)\n",
      "Requirement already satisfied: openai~=2.14.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (2.14.0)\n",
      "Requirement already satisfied: pandas~=2.2.3 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: prometheus-client~=0.20.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 8)) (0.20.0)\n",
      "Requirement already satisfied: requests~=2.32.3 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (2.32.5)\n",
      "Requirement already satisfied: wget~=3.2 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 10)) (3.2)\n",
      "Requirement already satisfied: typer~=0.15.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11)) (0.15.4)\n",
      "Requirement already satisfied: seaborn~=0.13.2 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from -r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 12)) (0.13.2)\n",
      "Requirement already satisfied: filelock in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (3.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (2026.2.19)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from torch~=2.6.0->-r /workspace/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: networkx in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from torch~=2.6.0->-r /workspace/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from torch~=2.6.0->-r /workspace/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from torch~=2.6.0->-r /workspace/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from torch~=2.6.0->-r /workspace/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch~=2.6.0->-r /workspace/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from aiohttp~=3.9.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from aiohttp~=3.9.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from aiohttp~=3.9.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from aiohttp~=3.9.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1)) (6.7.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from aiohttp~=3.9.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: iniconfig in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from pytest~=8.3.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from pytest~=8.3.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.34.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from matplotlib~=3.10.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from matplotlib~=3.10.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from matplotlib~=3.10.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from matplotlib~=3.10.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from matplotlib~=3.10.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (12.1.1)\n",
      "Requirement already satisfied: pyparsing>=3 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from matplotlib~=3.10.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from matplotlib~=3.10.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (2.12.5)\n",
      "Requirement already satisfied: sniffio in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from pandas~=2.2.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from pandas~=2.2.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 7)) (2025.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from requests~=2.32.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from requests~=2.32.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from requests~=2.32.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from requests~=2.32.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (2026.1.4)\n",
      "Requirement already satisfied: click<8.2,>=8.0.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from typer~=0.15.1->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11)) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from typer~=0.15.1->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from typer~=0.15.1->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11)) (14.3.3)\n",
      "Requirement already satisfied: httpcore==1.* in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai~=2.14.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (0.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from rich>=10.11.0->typer~=0.15.1->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from rich>=10.11.0->typer~=0.15.1->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r /workspace/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/.cache/pypoetry/virtualenvs/per-training-model-xS3fZVNL-py3.11/lib/python3.11/site-packages (from jinja2->torch~=2.6.0->-r /workspace/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.0.3)\n",
      "‚öôÔ∏è PASO 1: Convirtiendo HF a GGUF F16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: MiPM_Senior_HF\n",
      "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00005-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00006-of-00006.safetensors'\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.36.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.36.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.37.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.37.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.38.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.38.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.39.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.39.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.40.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.40.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.41.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.41.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> F16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.42.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.42.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.42.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.43.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.43.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.44.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.44.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.45.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.45.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.46.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.46.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.47.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.47.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 13824\n",
      "INFO:hf-to-gguf:gguf: head count = 40\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "The tokenizer you are loading from '/workspace/code/MiPM_Senior_HF' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151665\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/workspace/model_temp_f16.gguf: n_tensors = 579, total_size = 29.5G\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5G/29.5G [02:53<00:00, 170Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /workspace/model_temp_f16.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversi√≥n a F16 completada.\n",
      "üõ†Ô∏è PASO 2: Compilando llama-quantize con CMake...\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The ASM compiler identification is GNU\n",
      "-- Found assembler: /usr/bin/cc\n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.4.131\") \n",
      "-- CUDA Toolkit found\n",
      "-- The CUDA compiler identification is NVIDIA 12.4.131\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "-- Using CMAKE_CUDA_ARCHITECTURES=50-virtual;61-virtual;70-virtual;75-virtual;80-virtual;86-real;89-real CMAKE_CUDA_ARCHITECTURES_NATIVE=\n",
      "-- CUDA host compiler is GNU 11.4.0\n",
      "-- Including CUDA backend\n",
      "-- ggml version: 0.9.7\n",
      "-- ggml commit:  10b26ee23\n",
      "-- Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the system variable OPENSSL_ROOT_DIR (missing: OPENSSL_CRYPTO_LIBRARY OPENSSL_INCLUDE_DIR) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mCMake Warning at vendor/cpp-httplib/CMakeLists.txt:150 (message):\n",
      "  OpenSSL not found, HTTPS support disabled\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Generating embedded license file for target: common\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /workspace/code/llama.cpp/build\n",
      "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  1%] Built target build_info\n",
      "[  3%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  3%] Built target ggml-base\n",
      "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o\u001b[0m\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
      "[ 46%] Built target cpp-httplib\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[ 46%] Built target ggml-cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda-12.4/nvvm/bin/cicc: IO error: No space left on device\n",
      "ptxas fatal   : Input file '/tmp/tmpxft_00004021_00000000-6_fattn-mma-f16-instance-ncols1_2-ncols2_16.compute_89.ptx' could not be opened\n",
      "ptxas fatal   : Ptx assembly aborted due to errors\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1126: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o] Error 255\n",
      "gmake[3]: *** Waiting for unfinished jobs....\n",
      "/usr/local/cuda-12.4/nvvm/bin/cicc: IO error: No space left on device\n",
      "ptxas fatal   : Input file '/tmp/tmpxft_00003fdf_00000000-7_mmf-instance-ncols_6.compute_86.ptx' could not be opened\n",
      "ptxas fatal   : Ptx assembly aborted due to errors\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1770: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o] Error 255\n",
      "/usr/local/cuda-12.4/nvvm/bin/cicc: IO error: No space left on device\n",
      "Catastrophic error: cannot open generated C file \"/tmp/tmpxft_00004041_00000000-9_fattn-tile-instance-dkq72-dv72.compute_75.cudafe1.c\": No space left on device\n",
      "\n",
      "1 catastrophic error detected in the compilation of \"/workspace/code/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq72-dv72.cu\".\n",
      "Compilation terminated.\n",
      "Catastrophic error: cannot open generated C file \"/tmp/tmpxft_00004061_00000000-9_fattn-tile-instance-dkq96-dv96.compute_75.cudafe1.c\": No space left on device\n",
      "\n",
      "1 catastrophic error detected in the compilation of \"/workspace/code/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq96-dv96.cu\".\n",
      "Compilation terminated.\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1000: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o] Error 1\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1028: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o] Error 1\n",
      "/usr/local/cuda-12.4/nvvm/bin/cicc: IO error: No space left on device\n",
      "ptxas fatal   : Output file '/tmp/tmpxft_00003fae_00000000-21_sum.compute_89.cubin' could not be opened\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:790: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o] Error 255\n",
      "Segmentation fault (core dumped)\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1434: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o] Error 139\n",
      "/usr/include/c++/11/bits/move.h(105): catastrophic error: error while writing generated C file: No space left on device\n",
      "\n",
      "1 catastrophic error detected in the compilation of \"/workspace/code/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq128-dv128.cu\".\n",
      "Compilation terminated.\n",
      "/usr/local/cuda-12.4/nvvm/bin/cicc: IO error: No space left on device\n",
      "ptxas fatal   : Output file '/tmp/tmpxft_00004067_00000000-20_mmf-instance-ncols_7.compute_86.cubin' could not be opened\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:930: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o] Error 1\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1784: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o] Error 255\n",
      "/workspace/code/llama.cpp/ggml/src/ggml-cuda/mmvf.cu(14): catastrophic error: error while writing generated C file: No space left on device\n",
      "\n",
      "1 catastrophic error detected in the compilation of \"/workspace/code/llama.cpp/ggml/src/ggml-cuda/mmvf.cu\".\n",
      "Compilation terminated.\n",
      "Catastrophic error: cannot open generated C file \"/tmp/tmpxft_00003fcd_00000000-6_mmf-instance-ncols_13.compute_89.cudafe1.c\": No space left on device\n",
      "\n",
      "1 catastrophic error detected in the compilation of \"/workspace/code/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_13.cu\".\n",
      "Compilation terminated.\n",
      "Catastrophic error: cannot open generated C file \"/tmp/tmpxft_00003fc8_00000000-6_mmf-instance-ncols_10.compute_89.cudafe1.c\": No space left on device\n",
      "\n",
      "1 catastrophic error detected in the compilation of \"/workspace/code/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_10.cu\".\n",
      "Compilation terminated.\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1658: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o] Error 1\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1616: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o] Error 1\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:510: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o] Error 1\n",
      "In file included from tmpxft_00004090_00000000-6_mmf-instance-ncols_3.compute_89.cudafe1.stub.c:1:\n",
      "/tmp/tmpxft_00004090_00000000-6_mmf-instance-ncols_3.compute_89.cudafe1.stub.c:1119:27: fatal error: error writing to /tmp/cciXXhQm.s: No space left on device\n",
      " 1119 | #pragma GCC diagnostic pop\n",
      "      |                           ^\n",
      "compilation terminated.\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1728: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o] Error 1\n",
      "In file included from tmpxft_00004052_00000000-6_mmf-instance-ncols_1.compute_89.cudafe1.stub.c:1:\n",
      "/tmp/tmpxft_00004052_00000000-6_mmf-instance-ncols_1.compute_89.cudafe1.stub.c:1119:27: fatal error: error writing to /tmp/ccPJdbtY.s: No space left on device\n",
      " 1119 | #pragma GCC diagnostic pop\n",
      "      |                           ^\n",
      "compilation terminated.\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1602: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o] Error 1\n",
      "fatbinary fatal   : Could not open input file '/tmp/tmpxft_00003fd5_00000000-8_mmf-instance-ncols_16.compute_80.ptx'\n",
      "gmake[3]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1700: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o] Error 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def convert_and_quantize_manual(model_path, output_final_gguf):\n",
    "    print(f\"üöÄ Iniciando proceso para: {model_path}\")\n",
    "    \n",
    "    # 1. Clonar llama.cpp si no existe\n",
    "    if not os.path.exists(\"llama.cpp\"):\n",
    "        print(\"üì• Clonando llama.cpp...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\"], check=True)\n",
    "    \n",
    "    # 2. Instalar dependencias dentro de Poetry\n",
    "    print(\"üì¶ Asegurando dependencias en entorno Poetry...\")\n",
    "    subprocess.run([\"poetry\", \"run\", \"pip\", \"install\", \"gguf\", \"sentencepiece\", \"protobuf\"], check=True)\n",
    "    subprocess.run([\"poetry\", \"run\", \"pip\", \"install\", \"-r\", \"llama.cpp/requirements.txt\"], check=True)\n",
    "\n",
    "    # Rutas\n",
    "    temp_f16_gguf = \"/workspace/model_temp_f16.gguf\"\n",
    "    build_dir = \"llama.cpp/build\"\n",
    "    \n",
    "    try:\n",
    "        # PASO 1: Conversi√≥n HF -> GGUF F16\n",
    "        print(\"‚öôÔ∏è PASO 1: Convirtiendo HF a GGUF F16...\")\n",
    "        conv_script = \"llama.cpp/convert_hf_to_gguf.py\"\n",
    "        conv_cmd = [\n",
    "            \"poetry\", \"run\", \"python\", conv_script,\n",
    "            model_path,\n",
    "            \"--outfile\", temp_f16_gguf,\n",
    "            \"--outtype\", \"f16\"\n",
    "        ]\n",
    "        subprocess.run(conv_cmd, check=True)\n",
    "        print(\"‚úÖ Conversi√≥n a F16 completada.\")\n",
    "        \n",
    "        # PASO 2: Compilar usando CMake (Nuevo sistema)\n",
    "        print(\"üõ†Ô∏è PASO 2: Compilando llama-quantize con CMake...\")\n",
    "        os.makedirs(build_dir, exist_ok=True)\n",
    "        \n",
    "        # Configurar el build\n",
    "        subprocess.run([\"cmake\", \"-B\", build_dir, \"-S\", \"llama.cpp\", \"-DGGML_CUDA=ON\"], check=True)\n",
    "        \n",
    "        # Compilar solo el binario de cuantizaci√≥n (espec√≠ficamente el target 'llama-quantize')\n",
    "        subprocess.run([\"cmake\", \"--build\", build_dir, \"--config\", \"Release\", \"--target\", \"llama-quantize\", \"-j\"], check=True)\n",
    "        \n",
    "        # PASO 3: Cuantizaci√≥n de F16 a Q4_K_M\n",
    "        print(f\"üíé PASO 3: Cuantizando a q4_k_m -> {output_final_gguf}...\")\n",
    "        \n",
    "        # La ruta del binario ahora est√° dentro de build/bin/\n",
    "        quant_binary = os.path.join(build_dir, \"bin\", \"llama-quantize\")\n",
    "        \n",
    "        quant_cmd = [\n",
    "            quant_binary,\n",
    "            temp_f16_gguf,\n",
    "            output_final_gguf,\n",
    "            \"q4_k_m\"\n",
    "        ]\n",
    "        subprocess.run(quant_cmd, check=True)\n",
    "        \n",
    "        # Limpieza\n",
    "        print(\"üßπ Limpiando archivos temporales...\")\n",
    "        if os.path.exists(temp_f16_gguf):\n",
    "            os.remove(temp_f16_gguf)\n",
    "            \n",
    "        print(f\"üéä ¬°PROCESO EXITOSO! Descarga tu modelo aqu√≠: {output_final_gguf}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error en el comando: {e.cmd}\")\n",
    "        print(f\"C√≥digo de salida: {e.returncode}\")\n",
    "\n",
    "# --- EJECUCI√ìN ---\n",
    "input_hf_path = \"/workspace/code/MiPM_Senior_HF\"\n",
    "output_gguf_path = \"/workspace/MiPM_Senior_Expert.gguf\"\n",
    "\n",
    "convert_and_quantize_manual(input_hf_path, output_gguf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03cb66-929a-4f88-8fd0-46b07915f3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry - Product Manager Senior)",
   "language": "python",
   "name": "mi-pm-senior-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
