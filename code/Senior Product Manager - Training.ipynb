{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23c4f31-227b-4f9a-98d4-ba38f22ec8e1",
   "metadata": {},
   "source": [
    "### Carga del Modelo Base\n",
    "Vamos a traer a Qwen2.5-Coder-14B a la memoria de tu RTX 3090, comprimido en 4-bits para que entre sin problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cbf0e7-39e5-4609-b165-6ffc520b9768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/ollama-model-fine-tuning-sKOzwHPn-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Exec: poetry run python -m ipykernel install --user --name=per-training-model\n",
    "import os       # Permite interactuar con el sistema operativo (rutas de carpetas, variables de entorno).\n",
    "import gc       # \"Garbage Collector\": Se usa para liberar memoria RAM/VRAM manualmente si es necesario.\n",
    "import json     # Para manipular archivos JSON (lectura de datasets o configuraciones).\n",
    "import torch    # La librer√≠a principal de PyTorch para operaciones con tensores y uso de la GPU.\n",
    "import shutil   # √ötil para operaciones de archivos de alto nivel, como borrar o mover carpetas completas.\n",
    "import subprocess # Permite ejecutar comandos de terminal (como git o pip) desde Python.\n",
    "\n",
    "# Importa la clase Dataset de Hugging Face para estructurar los datos de entrenamiento.\n",
    "from datasets import Dataset \n",
    "\n",
    "# Unsloth: Librer√≠a optimizada para entrenar modelos m√°s r√°pido y con menos memoria.\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "# Permite obtener informaci√≥n t√©cnica de un modelo alojado en el Hugging Face Hub.\n",
    "from huggingface_hub import model_info\n",
    "\n",
    "# Facilita la aplicaci√≥n de formatos de chat (como Llama-3 o Alpaca) a los datos.\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# El \"Entrenador\" (Trainer) especializado en Supervised Fine-Tuning (SFT).\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Define los hiperpar√°metros del entrenamiento (√©pocas, tasa de aprendizaje, pasos, etc.).\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158f1dca-a9c0-4393-9aff-e9dfd1754beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Modelo NO encontrado en cach√©. Se descargar√° en: /root/.cache/huggingface/hub/models--unsloth--Qwen2.5-Coder-14B-Instruct-bnb-4bit\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  7.87s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo cargado exitosamente en 4-bits.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuraci√≥n de par√°metros iniciales ---\n",
    "\n",
    "# Define la longitud m√°xima de tokens (contexto) que el modelo procesar√°. \n",
    "# 2048 es est√°ndar, pero Unsloth permite ampliarlo din√°micamente.\n",
    "max_seq_length = 2048 \n",
    "\n",
    "# El tipo de datos para los pesos (None deja que Unsloth lo detecte autom√°ticamente).\n",
    "# Usualmente detectar√° float16 o bfloat16 seg√∫n tu GPU.\n",
    "dtype = None \n",
    "\n",
    "# Activa la cuantizaci√≥n de 4 bits. Crucial para que un modelo de 14B \n",
    "# quepa en GPUs de consumo (como una RTX 3060/4060 o superiores).\n",
    "load_in_4bit = True \n",
    "\n",
    "# El identificador del modelo en Hugging Face. \n",
    "# Esta versi√≥n ya viene pre-cuantizada (\"bnb-4bit\") para ser ultra r√°pida.\n",
    "model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\"\n",
    "\n",
    "# --- Verificaci√≥n de archivos locales (Cach√©) ---\n",
    "\n",
    "# Construye la ruta donde Hugging Face suele guardar los modelos descargados.\n",
    "# Transforma \"usuario/modelo\" en el formato de carpetas de cach√© del sistema.\n",
    "cache_dir = os.path.expanduser(f\"~/.cache/huggingface/hub/models--{'--'.join(model_name.split('/'))}\")\n",
    "\n",
    "# Comprueba si la carpeta del modelo ya existe en el disco duro.\n",
    "if os.path.exists(cache_dir):\n",
    "    # Si existe, nos avisa que no gastar√° internet descarg√°ndolo de nuevo.\n",
    "    print(f\"‚úÖ Modelo encontrado en cach√©: {cache_dir}\")\n",
    "    print(\"üîÑ Cargando modelo localmente...\")\n",
    "else:\n",
    "    # Si no existe, nos advierte que iniciar√° una descarga pesada.\n",
    "    print(f\"‚ö†Ô∏è Modelo NO encontrado en cach√©. Se descargar√° en: {cache_dir}\")\n",
    "\n",
    "# --- Carga del Modelo y el Tokenizador ---\n",
    "\n",
    "# Utiliza la funci√≥n optimizada de Unsloth para cargar el modelo en la VRAM de la GPU.\n",
    "# Retorna dos objetos: \n",
    "# 1. model: El cerebro del IA.\n",
    "# 2. tokenizer: El traductor que convierte texto en n√∫meros (tokens).\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# Confirmaci√≥n final de que el modelo est√° listo para usarse o entrenarse.\n",
    "print(\"‚úÖ Modelo cargado exitosamente en 4-bits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c773f-4058-4538-9ff8-d6ae84ee1a79",
   "metadata": {},
   "source": [
    "### Verificar que el modelo existe manualmente\n",
    "Si ninguna de las anteriores funciona, verifica que puedes acceder al modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0710312-e904-44c1-864b-b511e399badf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo encontrado: unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\n",
      "üì• Descargas: 8664\n"
     ]
    }
   ],
   "source": [
    "# --- Verificaci√≥n de metadatos en Hugging Face Hub ---\n",
    "\n",
    "# Usamos un bloque \"try-except\" para manejar posibles errores de conexi√≥n o permisos.\n",
    "try:\n",
    "    # Llama a la API de Hugging Face para obtener la ficha t√©cnica (info) del modelo.\n",
    "    # Esto no descarga el modelo, solo consulta sus estad√≠sticas y estado.\n",
    "    info = model_info(\"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\")\n",
    "    \n",
    "    # Si la consulta es exitosa, imprime el ID confirmado del modelo.\n",
    "    print(f\"‚úÖ Modelo encontrado: {info.id}\")\n",
    "    \n",
    "    # Muestra el n√∫mero total de descargas que ha tenido el modelo (popularidad).\n",
    "    print(f\"üì• Descargas: {info.downloads}\")\n",
    "\n",
    "# Si ocurre un error (ej. no hay internet, el modelo es privado o el nombre est√° mal escrito):\n",
    "except Exception as e:\n",
    "    # Atrapa el error y lo muestra en pantalla sin detener la ejecuci√≥n del programa.\n",
    "    print(f\"‚ùå Error accediendo al modelo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827c010-58fb-427a-8181-32708028502e",
   "metadata": {},
   "source": [
    "### Inyecci√≥n de Adaptadores (LoRA)\n",
    "Aqu√≠ definimos la arquitectura del fine-tuning. Solo vamos a entrenar una fracci√≥n del modelo (los adaptadores), lo que hace que el proceso sea r√°pido y eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff301f5-a741-40d4-a4b5-fa806bbf9150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.2.1 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Adaptadores LoRA configurados.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuraci√≥n de PEFT (Parameter-Efficient Fine-Tuning) con LoRA ---\n",
    "\n",
    "# Transforma el modelo base en un modelo PEFT (solo una parte es entrenable).\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    # 'r' (Rank): Define el tama√±o de las matrices de bajo rango. \n",
    "    # 16 es un equilibrio ideal entre precisi√≥n y ahorro de memoria.\n",
    "    r = 16, \n",
    "\n",
    "    # 'target_modules': Especifica en qu√© capas del modelo se inyectar√°n los adaptadores.\n",
    "    # Estas capas (q, k, v, o, gate, up, down) cubren casi toda la atenci√≥n y redes neuronales del modelo.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "\n",
    "    # 'lora_alpha': Escala el aprendizaje de los adaptadores. \n",
    "    # Generalmente se recomienda que sea igual o el doble de 'r'.\n",
    "    lora_alpha = 16,\n",
    "\n",
    "    # 'lora_dropout': Probabilidad de desactivar neuronas al azar para evitar sobreajuste.\n",
    "    # 0 es lo m√°s eficiente para velocidad de entrenamiento en Unsloth.\n",
    "    lora_dropout = 0, \n",
    "\n",
    "    # 'bias': Define si se entrenan los sesgos. \"none\" es lo est√°ndar para LoRA.\n",
    "    bias = \"none\",\n",
    "\n",
    "    # 'use_gradient_checkpointing': T√©cnica que libera memoria RAM de la GPU \n",
    "    # guardando solo lo esencial. \"unsloth\" usa una versi√≥n optimizada que gasta un 30% menos.\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "\n",
    "    # Semilla aleatoria para que los resultados sean reproducibles (siempre den lo mismo).\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# Mensaje de confirmaci√≥n: el modelo ahora est√° listo para recibir datos de entrenamiento.\n",
    "print(\"‚úÖ Adaptadores LoRA configurados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c7aeb-6101-4a9a-87e1-61c9692e44ed",
   "metadata": {},
   "source": [
    "#### Preparaci√≥n del Dataset\n",
    "Le ense√±amos al modelo a entender el formato de tu dataset.jsonl (ChatML: System, User, Assistant). Asumimos que dentro de tu JSONL, el arreglo de mensajes se llama messages o conversations.\n",
    "\n",
    "Este c√≥digo es fundamental: act√∫a como el traductor entre la forma en que t√∫ guardaste la informaci√≥n en tu archivo `epics.jsonl` y la forma exacta en la que el modelo (Qwen2.5) necesita leerla para aprender.\n",
    "\n",
    "En t√©rminos sencillos, el modelo no entiende \"columnas\" o \"diccionarios\"; solo entiende secuencias largas de texto con etiquetas especiales que le indican qui√©n est√° hablando.\n",
    "\n",
    "#### 1. Aplicar la plantilla ChatML (`get_chat_template`)\n",
    "\n",
    "Los modelos conversacionales como Qwen usan un formato llamado **ChatML** (Chat Markup Language). Esto significa que usan \"etiquetas invisibles\" para separar los mensajes, como `<|im_start|>user` y `<|im_end|>`.\n",
    "Este paso configura tu tokenizador para que inyecte autom√°ticamente estas etiquetas en el texto, ahorr√°ndote el trabajo de escribirlas a mano.\n",
    "\n",
    "#### 2. Definir el System Prompt\n",
    "\n",
    "Aqu√≠ le inyectamos la personalidad a tu PM Senior. Le estamos diciendo expl√≠citamente cu√°l es su rol y, muy importante, le indicamos que **debe responder en formato JSON**. Esto ancla el comportamiento del modelo para que siempre act√∫e como un experto estructurado.\n",
    "\n",
    "#### 3. La funci√≥n de formateo (`formatting_prompts_func`)\n",
    "\n",
    "Este es el \"motor\" de la celda. Como tu archivo tiene una columna llamada `input` y otra llamada `output`, la funci√≥n hace lo siguiente para cada fila de tu dataset:\n",
    "\n",
    "* **Extrae los datos:** Toma el diccionario crudo de `input` y el de `output`.\n",
    "* **Formatea a texto JSON legible (`json.dumps`):** Este paso es el truco vital. Como tu objetivo es que el modelo devuelva un JSON perfecto (como se ve en tus salidas con `epic_id`, `title`, `acceptance_criteria`), usamos `json.dumps(..., indent=2)` para transformar tus diccionarios en cadenas de texto con saltos de l√≠nea y tabulaciones perfectas. As√≠ el modelo aprende a indentar como un humano.\n",
    "* **Arma la conversaci√≥n:** Crea una lista l√≥gica con tres roles:\n",
    "1. El `system` (las instrucciones base).\n",
    "2. El `user` (tu `input` con el contexto y requerimientos).\n",
    "3. El `assistant` (tu `output` con la Epic dorada).\n",
    "\n",
    "\n",
    "* **Aplica la plantilla (`apply_chat_template`):** Pasa esa lista estructurada por el tokenizador para fusionarla en un √∫nico bloque de texto continuo con todas las etiquetas ChatML listas para el entrenamiento.\n",
    "\n",
    "#### 4. Cargar y procesar (`load_dataset` y `map`)\n",
    "\n",
    "* Usa la librer√≠a de Hugging Face para cargar tu archivo `epics.jsonl` a la memoria RAM de tu RunPod.\n",
    "* El comando `.map(..., batched=True)` pasa todo tu dataset por la funci√≥n que explicamos arriba de forma simult√°nea y s√∫per r√°pida.\n",
    "* El resultado final es que tu dataset ahora tiene una nueva columna llamada **`text`**, que contiene la conversaci√≥n perfectamente formateada. Esta columna `text` es la *√∫nica* que Qwen va a leer durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13f2245-b713-4332-aa75-511e7cce324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°Dataset cargado y formateado! Total de ejemplos procesados: 104\n",
      "\n",
      "--- MUESTRA DEL PRIMER EJEMPLO FORMATEADO ---\n",
      "<|im_start|>system\n",
      "Eres un Product Manager Senior experto. Tu tarea es analizar el contexto y los requerimientos proporcionados para redactar Epics de software detalladas, estructuradas y precisas en formato JSON.<|im_end|>\n",
      "<|im_start|>user\n",
      "{\n",
      "  \"context\": \"El proyecto inicia desde cero, sin ning√∫n tipo de infraestructura en la nube. Es imperativo establecer una base s√≥lida, repetible y segura que permita el despliegue y la operaci√≥n de todos los componentes subsecuentes de la plataforma. La adop...\n",
      "[CONTIN√öA]\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Preparaci√≥n del Formato de Conversaci√≥n ---\n",
    "\n",
    "# Configura el tokenizador para que use la estructura \"ChatML\" (<|im_start|>, <|im_end|>).\n",
    "# Esto es vital para que el modelo sepa cu√°ndo termina de hablar el usuario y empieza √©l.\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\",\n",
    ")\n",
    "\n",
    "# --- 2. Definici√≥n de la Identidad (System Prompt) ---\n",
    "\n",
    "# Establecemos las \"instrucciones de comportamiento\" del modelo. \n",
    "# Aqu√≠ le decimos que sea un Product Manager experto y que responda en JSON.\n",
    "system_prompt = \"Eres un Product Manager Senior experto. Tu tarea es analizar el contexto y los requerimientos proporcionados para redactar Epics de software detalladas, estructuradas y precisas en formato JSON.\"\n",
    "\n",
    "formatted_texts = []\n",
    "\n",
    "# --- 3. Procesamiento Manual del Archivo de Datos ---\n",
    "\n",
    "# Ruta donde tienes guardados tus ejemplos de entrenamiento (formato JSON Lines).\n",
    "file_path = \"../data/epics.jsonl\" \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        # Si la l√≠nea est√° vac√≠a, nos la saltamos para evitar errores.\n",
    "        if not line.strip(): continue \n",
    "        \n",
    "        # Convertimos la l√≠nea de texto (JSON) en un diccionario de Python.\n",
    "        record = json.loads(line)\n",
    "        \n",
    "        # Convertimos los campos 'input' y 'output' en texto (strings) bien formateados.\n",
    "        # 'ensure_ascii=False' permite tildes y √±; 'indent=2' lo hace legible.\n",
    "        user_content = json.dumps(record[\"input\"], ensure_ascii=False, indent=2)\n",
    "        assistant_content = json.dumps(record[\"output\"], ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Creamos la estructura de la conversaci√≥n (Mensaje de Sistema -> Usuario -> Asistente).\n",
    "        convo = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        \n",
    "        # 'apply_chat_template' une todo lo anterior usando las etiquetas especiales de ChatML.\n",
    "        # tokenize=False: Solo genera el texto plano por ahora.\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        \n",
    "        # Guardamos el resultado en una lista de diccionarios con la clave \"text\".\n",
    "        formatted_texts.append({\"text\": text})\n",
    "\n",
    "# --- 4. Creaci√≥n del Dataset Final ---\n",
    "\n",
    "# Convertimos nuestra lista de Python en un objeto Dataset de HuggingFace.\n",
    "# Este formato es el que el 'SFTTrainer' de Unsloth requiere para empezar a entrenar.\n",
    "dataset = Dataset.from_list(formatted_texts)\n",
    "\n",
    "# Mensajes de control para verificar que todo sali√≥ bien.\n",
    "print(f\"‚úÖ ¬°Dataset cargado y formateado! Total de ejemplos procesados: {len(dataset)}\")\n",
    "\n",
    "# Imprimimos los primeros 500 caracteres del primer ejemplo para ver las etiquetas <|im_start|>.\n",
    "print(\"\\n--- MUESTRA DEL PRIMER EJEMPLO FORMATEADO ---\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\\n[CONTIN√öA]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a428ef1-0453-4c79-8a4a-cefd7378a9fe",
   "metadata": {},
   "source": [
    "### **Fine-Tunning** (ajusta) un modelo de lenguaje pre-entrenado con tus datos espec√≠ficos usando **LoRA**\n",
    "\n",
    "| Componente | Funci√≥n |\n",
    "|------------|---------|\n",
    "| `SFTTrainer` | Entrenador especializado para \"Supervised Fine-Tuning\" (ajuste supervisado) |\n",
    "| `train_dataset` | Tus datos de entrenamiento (104 ejemplos conversacionales) |\n",
    "| `max_seq_length` | L√≠mite de tokens por ejemplo (2048) |\n",
    "| `packing=False` | Respeta la estructura conversacional exacta de cada ejemplo |\n",
    "| `per_device_train_batch_size=2` | Procesa 2 ejemplos simult√°neamente en GPU |\n",
    "| `gradient_accumulation_steps=4` | Simula un lote de 8 ejemplos (2√ó4) para ahorrar VRAM |\n",
    "| `num_train_epochs=3` | Pasa 3 veces por todo el dataset (~39 pasos totales) |\n",
    "| `learning_rate=2e-4` | Velocidad de aprendizaje est√°ndar para LoRA |\n",
    "| `adamw_8bit` | Optimizador comprimido que usa menos memoria |\n",
    "| `fp16/bf16` | Precisi√≥n mixta para acelerar entrenamiento |\n",
    "\n",
    "**Tiempo estimado:** ~39 pasos √ó tiempo por paso (var√≠a seg√∫n GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01333232-cdf6-408e-a226-05feefb79916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Cargando modelo base...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Inyectando adaptadores LoRA (Modo Ligero)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2026.2.1 patched 48 layers with 48 QKV layers, 48 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Formateando el dataset...\n",
      "üöÄ ¬°INICIANDO FINE-TUNING EXTREMO!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=64): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104/104 [00:15<00:00,  6.89 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 104 | Num Epochs = 3 | Total steps = 39\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 12,582,912 of 14,782,616,576 (0.09% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 02:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.836600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.777800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.478800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.458400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.506600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.526300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.383700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.346800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°Entrenamiento completado en 165.8158 segundos!\n"
     ]
    }
   ],
   "source": [
    "# --- 0. LIMPIEZA PROFUNDA DE GPU ---\n",
    "# Libera cualquier residuo de memoria en la GPU para empezar desde cero.\n",
    "torch.cuda.empty_cache() \n",
    "# Fuerza al recolector de basura de Python a limpiar objetos no utilizados en la RAM.\n",
    "gc.collect() \n",
    "\n",
    "# --- 1. CARGA DEL MODELO ---\n",
    "# Aumentamos la longitud de secuencia a 4096 para manejar Epics m√°s largas.\n",
    "max_seq_length = 4096 \n",
    "print(\"‚è≥ Cargando modelo base...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Cuantizaci√≥n para reducir el peso del modelo en VRAM.\n",
    ")\n",
    "\n",
    "# --- 2. ADAPTADORES LORA (MODO LIGERO) ---\n",
    "print(\"üß† Inyectando adaptadores LoRA (Modo Ligero)...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Reducimos el rango (Rank). Menos par√°metros entrenables = menos memoria.\n",
    "    # Solo entrenamos las capas de atenci√≥n (proyecciones Q, K, V, O).\n",
    "    # Al quitar \"gate_proj\", \"up_proj\" y \"down_proj\", ahorramos mucha VRAM.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# --- 3. PREPARAR DATASET ---\n",
    "# (Este bloque aplica la plantilla ChatML y limpia el JSONL como vimos anteriormente)\n",
    "print(\"üìä Formateando el dataset...\")\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"chatml\")\n",
    "system_prompt = \"Eres un Product Manager Senior experto...\"\n",
    "formatted_texts = []\n",
    "\n",
    "with open(\"../data/epics.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip(): continue\n",
    "        record = json.loads(line)\n",
    "        user_content = json.dumps(record[\"input\"], ensure_ascii=False, indent=2)\n",
    "        assistant_content = json.dumps(record[\"output\"], ensure_ascii=False, indent=2)\n",
    "        convo = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        formatted_texts.append({\"text\": text})\n",
    "\n",
    "dataset = Dataset.from_list(formatted_texts)\n",
    "\n",
    "# --- 4. ENTRENAMIENTO BLINDADO CONTRA OOM ---\n",
    "print(\"üöÄ ¬°INICIANDO FINE-TUNING EXTREMO!\")\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2, # Usa 2 procesos para cargar datos m√°s r√°pido.\n",
    "    packing = False,      # No empaqueta secuencias cortas (m√°s lento pero m√°s estable).\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1, # Procesa 1 ejemplo a la vez para no saturar la GPU.\n",
    "        gradient_accumulation_steps = 8, # Actualiza pesos cada 8 pasos (Batch efectivo = 8).\n",
    "        warmup_steps = 5,                # Sube la intensidad del aprendizaje gradualmente.\n",
    "        num_train_epochs = 3,            # El modelo ver√° el dataset completo 3 veces.\n",
    "        learning_rate = 2e-4,            # Velocidad de aprendizaje (est√°ndar para LoRA).\n",
    "        # Selecciona autom√°ticamente entre fp16 o bf16 seg√∫n la potencia de tu GPU.\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,               # Muestra el progreso en cada paso.\n",
    "        # \"paged_adamw_8bit\": Si la GPU se queda sin memoria, usa la RAM del sistema como \"colch√≥n\".\n",
    "        optim = \"paged_adamw_8bit\", \n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",          # Carpeta donde se guardar√°n los checkpoints.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Inicia el proceso de entrenamiento y guarda las estad√≠sticas finales.\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"‚úÖ ¬°Entrenamiento completado en {trainer_stats.metrics['train_runtime']} segundos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843da0b1-bc9e-4765-9fea-282216b45da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2361 kB]\n",
      "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6538 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [39.2 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3737 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1301 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6749 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4070 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1613 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
      "Fetched 47.0 MB in 3s (14.6 MB/s)                             \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "build-essential set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  cmake-data curl dh-elpa-helper emacsen-common libarchive13 libcurl4\n",
      "  libjsoncpp25 librhash0 libuv1\n",
      "Suggested packages:\n",
      "  cmake-doc ninja-build cmake-format lrzip libcurl4-doc libidn11-dev\n",
      "  libkrb5-dev libldap2-dev librtmp-dev libssh2-1-dev libssl-dev pkg-config\n",
      "  zlib1g-dev\n",
      "The following NEW packages will be installed:\n",
      "  cmake cmake-data dh-elpa-helper emacsen-common libarchive13\n",
      "  libcurl4-openssl-dev libjsoncpp25 librhash0 libuv1\n",
      "The following packages will be upgraded:\n",
      "  curl libcurl4\n",
      "2 upgraded, 9 newly installed, 0 to remove and 134 not upgraded.\n",
      "Need to get 8481 kB of archives.\n",
      "After this operation, 34.8 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libuv1 amd64 1.43.0-1ubuntu0.1 [92.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive13 amd64 3.6.0-1ubuntu1.5 [368 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 curl amd64 7.81.0-1ubuntu1.21 [194 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcurl4 amd64 7.81.0-1ubuntu1.21 [290 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjsoncpp25 amd64 1.9.5-3 [80.0 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 librhash0 amd64 1.4.2-1ubuntu1 [125 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 dh-elpa-helper all 2.0.9ubuntu1 [7610 B]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 emacsen-common all 3.0.4 [14.9 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 cmake-data all 3.22.1-1ubuntu1.22.04.2 [1913 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 cmake amd64 3.22.1-1ubuntu1.22.04.2 [5010 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcurl4-openssl-dev amd64 7.81.0-1ubuntu1.21 [386 kB]\n",
      "Fetched 8481 kB in 2s (5114 kB/s)              \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libuv1:amd64.\n",
      "(Reading database ... 24135 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libuv1_1.43.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libuv1:amd64 (1.43.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libarchive13:amd64.\n",
      "Preparing to unpack .../01-libarchive13_3.6.0-1ubuntu1.5_amd64.deb ...\n",
      "Unpacking libarchive13:amd64 (3.6.0-1ubuntu1.5) ...\n",
      "Preparing to unpack .../02-curl_7.81.0-1ubuntu1.21_amd64.deb ...\n",
      "Unpacking curl (7.81.0-1ubuntu1.21) over (7.81.0-1ubuntu1.18) ...\n",
      "Preparing to unpack .../03-libcurl4_7.81.0-1ubuntu1.21_amd64.deb ...\n",
      "Unpacking libcurl4:amd64 (7.81.0-1ubuntu1.21) over (7.81.0-1ubuntu1.18) ...\n",
      "Selecting previously unselected package libjsoncpp25:amd64.\n",
      "Preparing to unpack .../04-libjsoncpp25_1.9.5-3_amd64.deb ...\n",
      "Unpacking libjsoncpp25:amd64 (1.9.5-3) ...\n",
      "Selecting previously unselected package librhash0:amd64.\n",
      "Preparing to unpack .../05-librhash0_1.4.2-1ubuntu1_amd64.deb ...\n",
      "Unpacking librhash0:amd64 (1.4.2-1ubuntu1) ...\n",
      "Selecting previously unselected package dh-elpa-helper.\n",
      "Preparing to unpack .../06-dh-elpa-helper_2.0.9ubuntu1_all.deb ...\n",
      "Unpacking dh-elpa-helper (2.0.9ubuntu1) ...\n",
      "Selecting previously unselected package emacsen-common.\n",
      "Preparing to unpack .../07-emacsen-common_3.0.4_all.deb ...\n",
      "Unpacking emacsen-common (3.0.4) ...\n",
      "Selecting previously unselected package cmake-data.\n",
      "Preparing to unpack .../08-cmake-data_3.22.1-1ubuntu1.22.04.2_all.deb ...\n",
      "Unpacking cmake-data (3.22.1-1ubuntu1.22.04.2) ...\n",
      "Selecting previously unselected package cmake.\n",
      "Preparing to unpack .../09-cmake_3.22.1-1ubuntu1.22.04.2_amd64.deb ...\n",
      "Unpacking cmake (3.22.1-1ubuntu1.22.04.2) ...\n",
      "Selecting previously unselected package libcurl4-openssl-dev:amd64.\n",
      "Preparing to unpack .../10-libcurl4-openssl-dev_7.81.0-1ubuntu1.21_amd64.deb ...\n",
      "Unpacking libcurl4-openssl-dev:amd64 (7.81.0-1ubuntu1.21) ...\n",
      "Setting up libarchive13:amd64 (3.6.0-1ubuntu1.5) ...\n",
      "Setting up libuv1:amd64 (1.43.0-1ubuntu0.1) ...\n",
      "Setting up emacsen-common (3.0.4) ...\n",
      "Setting up dh-elpa-helper (2.0.9ubuntu1) ...\n",
      "Setting up libjsoncpp25:amd64 (1.9.5-3) ...\n",
      "Setting up librhash0:amd64 (1.4.2-1ubuntu1) ...\n",
      "Setting up libcurl4:amd64 (7.81.0-1ubuntu1.21) ...\n",
      "Setting up cmake-data (3.22.1-1ubuntu1.22.04.2) ...\n",
      "Setting up curl (7.81.0-1ubuntu1.21) ...\n",
      "Setting up libcurl4-openssl-dev:amd64 (7.81.0-1ubuntu1.21) ...\n",
      "Setting up cmake (3.22.1-1ubuntu1.22.04.2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n"
     ]
    }
   ],
   "source": [
    "# 'apt-get update -y': Actualiza la lista de paquetes disponibles en los repositorios.\n",
    "# El flag '-y' responde autom√°ticamente \"s√≠\" a las confirmaciones para que no se detenga.\n",
    "!apt-get update -y\n",
    "\n",
    "# 'apt-get install -y': Comando para instalar nuevos paquetes.\n",
    "# Se instalan 3 componentes fundamentales para compilar software:\n",
    "# 1. cmake: Herramienta avanzada para gestionar el proceso de compilaci√≥n...\n",
    "# 2. build-essential: Un paquete que incluye el compilador GCC, G++...\n",
    "# 3. libcurl4-openssl-dev: Librer√≠a necesaria para que las aplicaciones...\n",
    "!apt-get install -y cmake build-essential libcurl4-openssl-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb3b5cc1-12e6-4e7d-bd10-919692dbcab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  - \u001b[36mgguf\u001b[39m\n",
      "  - \u001b[36mprotobuf\u001b[39m\n",
      "  - \u001b[36msentencepiece\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "# '!': Ejecuta el comando en la consola del sistema.\n",
    "# 'poetry add': Instala las librer√≠as y las registra en tu archivo de proyecto.\n",
    "# 'gguf': Necesario para escribir y leer el formato de archivo final (.gguf).\n",
    "# 'protobuf': El sistema de serializaci√≥n de datos que usa Google y Hugging Face.\n",
    "# 'sentencepiece': ¬°La pieza clave! Es la librer√≠a que maneja la tokenizaci√≥n \n",
    "# de modelos como Qwen, Llama y Mistral. Sin esto, el modelo no puede \n",
    "# descomponer las palabras en unidades que entienda (tokens).\n",
    "!poetry add gguf protobuf sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22105179-cc95-4f07-a87b-7524542e690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '!': Indica que el comando se ejecuta en la consola del sistema (Shell).\n",
    "# 'rm': Es el comando para \"remover\" (borrar) archivos o directorios.\n",
    "\n",
    "!rm -rf llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "700780ac-532f-44f6-a310-b8d74d04a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Descargando llama.cpp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into './llama.cpp'...\n",
      "Updating files: 100% (2278/2278), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ llama.cpp descargado\n",
      "üì¶ Instalando dependencias...\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\n",
      "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
      "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
      "Collecting numpy~=1.26.4 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 1))\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting sentencepiece<0.3.0,>=0.1.98 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 2))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.57.1 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4))\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting gguf>=0.1.0 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 6))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 7))\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting torch~=2.6.0 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5))\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp311-cp311-linux_x86_64.whl.metadata (26 kB)\n",
      "Collecting aiohttp~=3.9.3 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytest~=8.3.3 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 2))\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting huggingface_hub<1.0,>=0.34.0 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 3))\n",
      "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting matplotlib~=3.10.0 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4))\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Collecting openai~=2.14.0 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pandas~=2.2.3 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 7))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m158.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting prometheus-client~=0.20.0 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 8))\n",
      "  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (2.32.3)\n",
      "Collecting wget~=3.2 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 10))\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting typer~=0.15.1 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11))\n",
      "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting seaborn~=0.13.2 (from -r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 12))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (3.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.57.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4))\n",
      "  Downloading regex-2026.2.19-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.57.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4))\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.57.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4))\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers<5.0.0,>=4.57.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4))\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch~=2.6.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5))\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2024.2.0)\n",
      "Collecting sympy==1.13.1 (from torch~=2.6.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch~=2.6.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.3.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp~=3.9.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1)) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp~=3.9.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1))\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp~=3.9.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1))\n",
      "  Downloading multidict-6.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp~=3.9.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1))\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting iniconfig (from pytest~=8.3.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 2))\n",
      "  Downloading iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest~=8.3.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 2))\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub<1.0,>=0.34.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 3))\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.10.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4))\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.10.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.10.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4))\n",
      "  Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib~=3.10.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4))\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (10.2.0)\n",
      "Collecting pyparsing>=3 (from matplotlib~=3.10.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4))\n",
      "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (0.27.2)\n",
      "Collecting jiter<1,>=0.10.0 (from openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading jiter-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
      "Collecting pytz>=2020.1 (from pandas~=2.2.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 7))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas~=2.2.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 7))\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 9)) (2024.8.30)\n",
      "Collecting click<8.2,>=8.0.0 (from typer~=0.15.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
      "Collecting rich>=10.11.0 (from typer~=0.15.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11))\n",
      "  Downloading rich-14.3.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6)) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai~=2.14.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting propcache>=0.2.1 (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 1))\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 4)) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer~=0.15.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11)) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-tool_bench.txt (line 11))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch~=2.6.0->-r /workspace/ollama-model-fine-tuning/code/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2.1.5)\n",
      "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp311-cp311-linux_x86_64.whl (178.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m178.7/178.7 MB\u001b[0m \u001b[31m192.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "Downloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
      "Downloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (362 kB)\n",
      "Downloading multidict-6.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "Downloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Downloading https://download.pytorch.org/whl/nightly/aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading https://download.pytorch.org/whl/nightly/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "Downloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Downloading https://download.pytorch.org/whl/nightly/pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2026.2.19-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m800.6/800.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.3.3-py3-none-any.whl (310 kB)\n",
      "Downloading https://download.pytorch.org/whl/nightly/markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading https://download.pytorch.org/whl/nightly/mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (pyproject.toml): started\n",
      "  Building wheel for wget (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9686 sha256=da1a17d37ed1d3b695084fa285ecac6f1172a67583f5678805a400a31d2bf0b4\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, pytz, tzdata, typing-extensions, tqdm, sympy, sentencepiece, safetensors, regex, pyparsing, protobuf, propcache, prometheus-client, pluggy, numpy, multidict, mdurl, kiwisolver, jiter, iniconfig, hf-xet, frozenlist, fonttools, cycler, click, annotated-types, yarl, typing-inspection, torch, pytest, pydantic-core, pandas, markdown-it-py, huggingface_hub, gguf, contourpy, aiosignal, tokenizers, rich, pydantic, matplotlib, aiohttp, typer, transformers, seaborn, openai\n",
      "\u001b[2K  Attempting uninstall: typing-extensions‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 1/46\u001b[0m [pytz]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.9.0m \u001b[32m 1/46\u001b[0m [pytz]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.9.0:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 1/46\u001b[0m [pytz]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.9.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/46\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: sympy‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/46\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: sympy 1.12‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/46\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling sympy-1.12:90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 5/46\u001b[0m [sympy]ensions]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.12‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 5/46\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: pyparsing‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 7/46\u001b[0m [safetensors]\n",
      "\u001b[2K    Found existing installation: pyparsing 2.4.7‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 7/46\u001b[0m [safetensors]\n",
      "\u001b[2K    Uninstalling pyparsing-2.4.7:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 7/46\u001b[0m [safetensors]\n",
      "\u001b[2K      Successfully uninstalled pyparsing-2.4.7‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 7/46\u001b[0m [safetensors]\n",
      "\u001b[2K  Attempting uninstall: prometheus-client‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 9/46\u001b[0m [pyparsing]\n",
      "\u001b[2K    Found existing installation: prometheus_client 0.21.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 9/46\u001b[0m [pyparsing]\n",
      "\u001b[2K    Uninstalling prometheus_client-0.21.0:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 9/46\u001b[0m [pyparsing]\n",
      "\u001b[2K      Successfully uninstalled prometheus_client-0.21.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12/46\u001b[0m [prometheus-client]\n",
      "\u001b[2K  Attempting uninstall: numpy\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13/46\u001b[0m [pluggy]client]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.3‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13/46\u001b[0m [pluggy]\n",
      "\u001b[2K    Uninstalling numpy-1.26.3:[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13/46\u001b[0m [pluggy]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.3‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14/46\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: torch\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m25/46\u001b[0m [annotated-types]\n",
      "\u001b[2K    Found existing installation: torch 2.4.1+cu124‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m25/46\u001b[0m [annotated-types]\n",
      "\u001b[2K    Uninstalling torch-2.4.1+cu124:m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m28/46\u001b[0m [torch]types]\n",
      "\u001b[2K      Successfully uninstalled torch-2.4.1+cu12490m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m28/46\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46/46\u001b[0m [openai] [openai] [seaborn]mers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohttp-3.9.5 aiosignal-1.4.0 annotated-types-0.7.0 click-8.1.8 contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 frozenlist-1.8.0 gguf-0.17.1 hf-xet-1.2.0 huggingface_hub-0.36.2 iniconfig-2.3.0 jiter-0.13.0 kiwisolver-1.4.9 markdown-it-py-4.0.0 matplotlib-3.10.8 mdurl-0.1.2 multidict-6.7.1 numpy-1.26.4 openai-2.14.0 pandas-2.2.3 pluggy-1.6.0 prometheus-client-0.20.0 propcache-0.4.1 protobuf-4.25.8 pydantic-2.12.5 pydantic-core-2.41.5 pyparsing-3.3.2 pytest-8.3.5 pytz-2025.2 regex-2026.2.19 rich-14.3.3 safetensors-0.7.0 seaborn-0.13.2 sentencepiece-0.2.1 sympy-1.13.1 tokenizers-0.22.2 torch-2.6.0+cpu tqdm-4.67.3 transformers-4.57.6 typer-0.15.4 typing-extensions-4.15.0 typing-inspection-0.4.2 tzdata-2025.3 wget-3.2 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.4.1+cu124 requires torch==2.4.1, but you have torch 2.6.0+cpu which is incompatible.\n",
      "torchvision 0.19.1+cu124 requires torch==2.4.1, but you have torch 2.6.0+cpu which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Iniciando fusi√≥n y exportaci√≥n a GGUF...\n",
      "‚è≥ Esto puede tomar 10-20 minutos...\n",
      "‚ö†Ô∏è Error con m√©todo autom√°tico: unsloth_save_pretrained_gguf() got an unexpected keyword argument 'converter_location'\n",
      "üîÑ Intentando m√©todo manual...\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00006.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [01:26<00:00, 14.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [01:57<00:00, 19.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/ollama-model-fine-tuning/code/MiPM_Senior_HF`\n",
      "‚úÖ Modelo guardado en formato HF\n",
      "üìù Para convertir a GGUF manualmente, ejecuta en terminal:\n",
      "\n",
      "    cd ./llama.cpp\n",
      "    python convert_hf_to_gguf.py ../MiPM_Senior_HF --outfile ../MiPM_Senior.gguf --outtype q4_k_m\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# --- Verificaci√≥n y Setup de llama.cpp ---\n",
    "\n",
    "# Definimos la ruta local donde queremos que viva el repositorio llama.cpp.\n",
    "LLAMA_CPP_DIR = \"./llama.cpp\"\n",
    "\n",
    "# Comprobamos si la carpeta ya existe para no descargarla dos veces.\n",
    "if not os.path.exists(LLAMA_CPP_DIR):\n",
    "    print(\"üì• Descargando llama.cpp...\")\n",
    "    # 'git clone': Descarga el c√≥digo fuente oficial de llama.cpp.\n",
    "    subprocess.run([\n",
    "        \"git\", \"clone\", \n",
    "        \"https://github.com/ggerganov/llama.cpp.git\",\n",
    "        LLAMA_CPP_DIR\n",
    "    ], check=True)\n",
    "    print(\"‚úÖ llama.cpp descargado\")\n",
    "    \n",
    "    # Instalamos las librer√≠as de Python necesarias para que los scripts de conversi√≥n funcionen.\n",
    "    print(\"üì¶ Instalando dependencias...\")\n",
    "    subprocess.run([\n",
    "        \"pip\", \"install\", \"-r\", \n",
    "        f\"{LLAMA_CPP_DIR}/requirements.txt\"\n",
    "    ], check=True)\n",
    "else:\n",
    "    # Si la carpeta ya est√° ah√≠, simplemente lo confirmamos.\n",
    "    print(\"‚úÖ llama.cpp ya existe\")\n",
    "\n",
    "# --- Proceso de Exportaci√≥n ---\n",
    "\n",
    "print(\"üì¶ Iniciando fusi√≥n y exportaci√≥n a GGUF...\")\n",
    "print(\"‚è≥ Esto puede tomar 10-20 minutos...\")\n",
    "\n",
    "try:\n",
    "    # Intento A: Usar la funci√≥n integrada de Unsloth.\n",
    "    model.save_pretrained_gguf(\n",
    "        \"MiPM_Senior\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_m\", # Cuantizaci√≥n de 4 bits (calidad media-alta).\n",
    "        # Indicamos expl√≠citamente d√≥nde est√° la herramienta de conversi√≥n.\n",
    "        converter_location=LLAMA_CPP_DIR,\n",
    "    )\n",
    "    print(\"‚úÖ ¬°PROCESO COMPLETADO!\")\n",
    "    print(\"üìÅ Archivo generado: MiPM_Senior_q4_k_m.gguf\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Intento B (Fallback): Si lo anterior falla (por errores de memoria o librer√≠as),\n",
    "    # hacemos el proceso en dos pasos manuales.\n",
    "    print(f\"‚ö†Ô∏è Error con m√©todo autom√°tico: {e}\")\n",
    "    print(\"üîÑ Intentando m√©todo manual...\")\n",
    "    \n",
    "    # 1. Primero, fusionamos el modelo LoRA con el base y lo guardamos como un modelo normal de Hugging Face.\n",
    "    # 'merged_16bit' asegura que no haya p√©rdida de calidad en esta fase.\n",
    "    model.save_pretrained_merged(\n",
    "        \"MiPM_Senior_HF\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\",\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Modelo guardado en formato HF\")\n",
    "    # 2. Instrucciones para que el usuario ejecute la conversi√≥n final desde la consola.\n",
    "    print(\"üìù Para convertir a GGUF manualmente, ejecuta en terminal:\")\n",
    "    print(f\"\"\"\n",
    "    cd {LLAMA_CPP_DIR}\n",
    "    python convert_hf_to_gguf.py ../MiPM_Senior_HF --outfile ../MiPM_Senior.gguf --outtype q4_k_m\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "239a5721-0d86-4640-83fb-25346d729775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease               \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \n",
      "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
      "g++ set to manually installed.\n",
      "gcc is already the newest version (4:11.2.0-1ubuntu1).\n",
      "gcc set to manually installed.\n",
      "make is already the newest version (4.3-4.1build1).\n",
      "make set to manually installed.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 134 not upgraded.\n",
      "\n",
      "--- ESPACIO EN DISCO ---\n",
      "Filesystem                   Size  Used Avail Use% Mounted on\n",
      "mfs#us-il-1.runpod.net:9421  503T  355T  149T  71% /workspace\n"
     ]
    }
   ],
   "source": [
    "# Instalar herramientas de compilaci√≥n esenciales\n",
    "!apt-get update && apt-get install -y build-essential gcc g++ make\n",
    "\n",
    "# Verificar espacio en disco en /workspace\n",
    "print(\"\\n--- ESPACIO EN DISCO ---\")\n",
    "!df -h /workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0afc2c69-dcae-4183-b41e-150b9821eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /workspace/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a60f8fe-aea8-4761-bb57-9479e323b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Limpiando rastros para recuperar cuota de sistema...\n",
      "‚öôÔ∏è PASO 1: Convirtiendo HF a GGUF F16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: MiPM_Senior_HF\n",
      "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00005-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00006-of-00006.safetensors'\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.36.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.36.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.37.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.37.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.38.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.38.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.39.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.39.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.40.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.40.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.41.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.41.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> F16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.42.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.42.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.42.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.43.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.43.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.44.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.44.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.45.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.45.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.46.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.46.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.47.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.47.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 13824\n",
      "INFO:hf-to-gguf:gguf: head count = 40\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "The tokenizer you are loading from '/workspace/ollama-model-fine-tuning/code/MiPM_Senior_HF' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151665\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/workspace/model_temp_f16.gguf: n_tensors = 579, total_size = 29.5G\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5G/29.5G [03:24<00:00, 144Mbyte/s] \n",
      "INFO:hf-to-gguf:Model successfully exported to /workspace/model_temp_f16.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è PASO 2: Compilando localmente en /workspace...\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The ASM compiler identification is GNU\n",
      "-- Found assembler: /usr/bin/cc\n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- ggml version: 0.9.7\n",
      "-- ggml commit:  b908baf18-dirty\n",
      "-- Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the system variable OPENSSL_ROOT_DIR (missing: OPENSSL_CRYPTO_LIBRARY OPENSSL_INCLUDE_DIR) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mCMake Warning at vendor/cpp-httplib/CMakeLists.txt:150 (message):\n",
      "  OpenSSL not found, HTTPS support disabled\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Generating embedded license file for target: common\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /workspace/llama_build_final\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  4%] Built target build_info\n",
      "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  4%] Built target ggml-base\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[ 12%] Built target ggml-cpu\n",
      "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "[ 14%] Built target ggml\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampler.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/delta-net-base.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais2.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba-base.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/paddleocr.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen35.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
      "[ 82%] Built target cpp-httplib\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "[ 82%] Built target llama\n",
      "[ 82%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/debug.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-mod.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-map.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/parser.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/lexer.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/string.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/value.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/__/license.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/runtime.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/caps.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 97%] Built target common\n",
      "[ 97%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "[100%] Built target llama-quantize\n",
      "üíé PASO 3: Cuantizando a q4_k_m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 8117 (b908baf18)\n",
      "main: built with GNU 11.4.0 for Linux x86_64\n",
      "main: quantizing '/workspace/model_temp_f16.gguf' to '/workspace/ollama-model-fine-tuning/MiPM_Senior_Final.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 579 tensors from /workspace/model_temp_f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = MiPM_Senior_HF\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 15B\n",
      "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 48\n",
      "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151665\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - type  f32:  241 tensors\n",
      "llama_model_loader: - type  f16:  338 tensors\n",
      "[   1/ 579]                        output.weight - [  5120, 152064,      1,      1], type =    f16, converting to q6_K .. size =  1485.00 MiB ->   609.08 MiB\n",
      "[   2/ 579]                   output_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[   3/ 579]                    token_embd.weight - [  5120, 152064,      1,      1], type =    f16, converting to q4_K .. size =  1485.00 MiB ->   417.66 MiB\n",
      "[   4/ 579]                    blk.0.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[   5/ 579]                  blk.0.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[   6/ 579]               blk.0.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[   7/ 579]             blk.0.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[   8/ 579]                    blk.0.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[   9/ 579]                  blk.0.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  10/ 579]                    blk.0.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  11/ 579]                  blk.0.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  12/ 579]                blk.0.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[  13/ 579]                blk.0.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  14/ 579]                blk.0.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  15/ 579]                  blk.0.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  16/ 579]                    blk.1.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  17/ 579]                  blk.1.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  18/ 579]               blk.1.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  19/ 579]             blk.1.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  20/ 579]                    blk.1.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  21/ 579]                  blk.1.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  22/ 579]                    blk.1.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  23/ 579]                  blk.1.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  24/ 579]                blk.1.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[  25/ 579]                blk.1.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  26/ 579]                blk.1.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  27/ 579]                  blk.1.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  28/ 579]                    blk.2.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  29/ 579]                  blk.2.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  30/ 579]               blk.2.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  31/ 579]             blk.2.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  32/ 579]                    blk.2.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  33/ 579]                  blk.2.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  34/ 579]                    blk.2.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  35/ 579]                  blk.2.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  36/ 579]                blk.2.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[  37/ 579]                blk.2.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  38/ 579]                blk.2.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  39/ 579]                  blk.2.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  40/ 579]                    blk.3.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  41/ 579]                  blk.3.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  42/ 579]               blk.3.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  43/ 579]             blk.3.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  44/ 579]                    blk.3.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  45/ 579]                  blk.3.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  46/ 579]                    blk.3.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  47/ 579]                  blk.3.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  48/ 579]                blk.3.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[  49/ 579]                blk.3.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  50/ 579]                blk.3.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  51/ 579]                  blk.3.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  52/ 579]                    blk.4.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  53/ 579]                  blk.4.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  54/ 579]               blk.4.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  55/ 579]             blk.4.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  56/ 579]                    blk.4.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  57/ 579]                  blk.4.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  58/ 579]                    blk.4.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  59/ 579]                  blk.4.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  60/ 579]                blk.4.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[  61/ 579]                blk.4.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  62/ 579]                blk.4.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  63/ 579]                  blk.4.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  64/ 579]                    blk.5.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  65/ 579]                  blk.5.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  66/ 579]               blk.5.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  67/ 579]             blk.5.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  68/ 579]                    blk.5.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  69/ 579]                  blk.5.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  70/ 579]                    blk.5.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  71/ 579]                  blk.5.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[  72/ 579]                blk.5.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[  73/ 579]                blk.5.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  74/ 579]                blk.5.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  75/ 579]                  blk.5.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  76/ 579]                    blk.6.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  77/ 579]                  blk.6.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  78/ 579]               blk.6.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  79/ 579]             blk.6.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  80/ 579]                    blk.6.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  81/ 579]                  blk.6.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  82/ 579]                    blk.6.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  83/ 579]                  blk.6.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  84/ 579]                blk.6.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  85/ 579]                blk.6.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  86/ 579]                blk.6.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  87/ 579]                  blk.6.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  88/ 579]                    blk.7.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  89/ 579]                  blk.7.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  90/ 579]               blk.7.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  91/ 579]             blk.7.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  92/ 579]                    blk.7.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  93/ 579]                  blk.7.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[  94/ 579]                    blk.7.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[  95/ 579]                  blk.7.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[  96/ 579]                blk.7.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  97/ 579]                blk.7.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[  98/ 579]                blk.7.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[  99/ 579]                  blk.7.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 100/ 579]                    blk.8.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 101/ 579]                  blk.8.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 102/ 579]               blk.8.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 103/ 579]             blk.8.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 104/ 579]                    blk.8.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 105/ 579]                  blk.8.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 106/ 579]                    blk.8.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 107/ 579]                  blk.8.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 108/ 579]                blk.8.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 109/ 579]                blk.8.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 110/ 579]                blk.8.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 111/ 579]                  blk.8.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 112/ 579]                    blk.9.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 113/ 579]                  blk.9.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 114/ 579]               blk.9.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 115/ 579]             blk.9.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 116/ 579]                    blk.9.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 117/ 579]                  blk.9.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 118/ 579]                    blk.9.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 119/ 579]                  blk.9.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 120/ 579]                blk.9.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 121/ 579]                blk.9.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 122/ 579]                blk.9.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 123/ 579]                  blk.9.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 124/ 579]                   blk.10.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 125/ 579]                 blk.10.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 126/ 579]              blk.10.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 127/ 579]            blk.10.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 128/ 579]                   blk.10.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 129/ 579]                 blk.10.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 130/ 579]                   blk.10.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 131/ 579]                 blk.10.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 132/ 579]               blk.10.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 133/ 579]               blk.10.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 134/ 579]               blk.10.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 135/ 579]                 blk.10.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 136/ 579]                   blk.11.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 137/ 579]                 blk.11.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 138/ 579]              blk.11.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 139/ 579]            blk.11.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 140/ 579]                   blk.11.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 141/ 579]                 blk.11.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 142/ 579]                   blk.11.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 143/ 579]                 blk.11.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 144/ 579]               blk.11.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 145/ 579]               blk.11.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 146/ 579]               blk.11.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 147/ 579]                 blk.11.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 148/ 579]                   blk.12.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 149/ 579]                 blk.12.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 150/ 579]              blk.12.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 151/ 579]            blk.12.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 152/ 579]                   blk.12.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 153/ 579]                 blk.12.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 154/ 579]                   blk.12.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 155/ 579]                 blk.12.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 156/ 579]               blk.12.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 157/ 579]               blk.12.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 158/ 579]               blk.12.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 159/ 579]                 blk.12.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 160/ 579]                   blk.13.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 161/ 579]                 blk.13.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 162/ 579]              blk.13.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 163/ 579]            blk.13.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 164/ 579]                   blk.13.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 165/ 579]                 blk.13.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 166/ 579]                   blk.13.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 167/ 579]                 blk.13.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 168/ 579]               blk.13.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 169/ 579]               blk.13.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 170/ 579]               blk.13.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 171/ 579]                 blk.13.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 172/ 579]                   blk.14.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 173/ 579]                 blk.14.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 174/ 579]              blk.14.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 175/ 579]            blk.14.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 176/ 579]                   blk.14.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 177/ 579]                 blk.14.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 178/ 579]                   blk.14.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 179/ 579]                 blk.14.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 180/ 579]               blk.14.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 181/ 579]               blk.14.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 182/ 579]               blk.14.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 183/ 579]                 blk.14.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 184/ 579]                   blk.15.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 185/ 579]                 blk.15.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 186/ 579]              blk.15.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 187/ 579]            blk.15.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 188/ 579]                   blk.15.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 189/ 579]                 blk.15.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 190/ 579]                   blk.15.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 191/ 579]                 blk.15.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 192/ 579]               blk.15.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 193/ 579]               blk.15.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 194/ 579]               blk.15.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 195/ 579]                 blk.15.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 196/ 579]                   blk.16.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 197/ 579]                 blk.16.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 198/ 579]              blk.16.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 199/ 579]            blk.16.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 200/ 579]                   blk.16.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 201/ 579]                 blk.16.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 202/ 579]                   blk.16.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 203/ 579]                 blk.16.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 204/ 579]               blk.16.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 205/ 579]               blk.16.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 206/ 579]               blk.16.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 207/ 579]                 blk.16.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 208/ 579]                   blk.17.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 209/ 579]                 blk.17.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 210/ 579]              blk.17.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 211/ 579]            blk.17.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 212/ 579]                   blk.17.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 213/ 579]                 blk.17.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 214/ 579]                   blk.17.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 215/ 579]                 blk.17.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 216/ 579]               blk.17.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 217/ 579]               blk.17.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 218/ 579]               blk.17.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 219/ 579]                 blk.17.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 220/ 579]                   blk.18.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 221/ 579]                 blk.18.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 222/ 579]              blk.18.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 223/ 579]            blk.18.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 224/ 579]                   blk.18.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 225/ 579]                 blk.18.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 226/ 579]                   blk.18.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 227/ 579]                 blk.18.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 228/ 579]               blk.18.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 229/ 579]               blk.18.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 230/ 579]               blk.18.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 231/ 579]                 blk.18.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 232/ 579]                   blk.19.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 233/ 579]                 blk.19.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 234/ 579]              blk.19.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 235/ 579]            blk.19.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 236/ 579]                   blk.19.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 237/ 579]                 blk.19.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 238/ 579]                   blk.19.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 239/ 579]                 blk.19.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 240/ 579]               blk.19.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 241/ 579]               blk.19.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 242/ 579]               blk.19.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 243/ 579]                 blk.19.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 244/ 579]                   blk.20.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 245/ 579]                 blk.20.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 246/ 579]              blk.20.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 247/ 579]            blk.20.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 248/ 579]                   blk.20.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 249/ 579]                 blk.20.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 250/ 579]                   blk.20.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 251/ 579]                 blk.20.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 252/ 579]               blk.20.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 253/ 579]               blk.20.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 254/ 579]               blk.20.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 255/ 579]                 blk.20.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 256/ 579]                   blk.21.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 257/ 579]                 blk.21.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 258/ 579]              blk.21.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 259/ 579]            blk.21.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 260/ 579]                   blk.21.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 261/ 579]                 blk.21.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 262/ 579]                   blk.21.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 263/ 579]                 blk.21.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 264/ 579]               blk.21.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 265/ 579]               blk.21.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 266/ 579]               blk.21.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 267/ 579]                 blk.21.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 268/ 579]                   blk.22.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 269/ 579]                 blk.22.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 270/ 579]              blk.22.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 271/ 579]            blk.22.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 272/ 579]                   blk.22.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 273/ 579]                 blk.22.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 274/ 579]                   blk.22.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 275/ 579]                 blk.22.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 276/ 579]               blk.22.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 277/ 579]               blk.22.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 278/ 579]               blk.22.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 279/ 579]                 blk.22.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 280/ 579]                   blk.23.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 281/ 579]                 blk.23.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 282/ 579]              blk.23.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 283/ 579]            blk.23.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 284/ 579]                   blk.23.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 285/ 579]                 blk.23.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 286/ 579]                   blk.23.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 287/ 579]                 blk.23.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 288/ 579]               blk.23.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 289/ 579]               blk.23.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 290/ 579]               blk.23.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 291/ 579]                 blk.23.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 292/ 579]                   blk.24.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 293/ 579]                 blk.24.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 294/ 579]              blk.24.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 295/ 579]            blk.24.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 296/ 579]                   blk.24.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 297/ 579]                 blk.24.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 298/ 579]                   blk.24.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 299/ 579]                 blk.24.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 300/ 579]               blk.24.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 301/ 579]               blk.24.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 302/ 579]               blk.24.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 303/ 579]                 blk.24.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 304/ 579]                   blk.25.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 305/ 579]                 blk.25.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 306/ 579]              blk.25.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 307/ 579]            blk.25.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 308/ 579]                   blk.25.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 309/ 579]                 blk.25.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 310/ 579]                   blk.25.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 311/ 579]                 blk.25.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 312/ 579]               blk.25.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 313/ 579]               blk.25.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 314/ 579]               blk.25.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 315/ 579]                 blk.25.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 316/ 579]                   blk.26.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 317/ 579]                 blk.26.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 318/ 579]              blk.26.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 319/ 579]            blk.26.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 320/ 579]                   blk.26.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 321/ 579]                 blk.26.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 322/ 579]                   blk.26.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 323/ 579]                 blk.26.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 324/ 579]               blk.26.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 325/ 579]               blk.26.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 326/ 579]               blk.26.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 327/ 579]                 blk.26.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 328/ 579]                   blk.27.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 329/ 579]                 blk.27.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 330/ 579]              blk.27.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 331/ 579]            blk.27.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 332/ 579]                   blk.27.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 333/ 579]                 blk.27.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 334/ 579]                   blk.27.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 335/ 579]                 blk.27.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 336/ 579]               blk.27.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 337/ 579]               blk.27.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 338/ 579]               blk.27.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 339/ 579]                 blk.27.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 340/ 579]                   blk.28.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 341/ 579]                 blk.28.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 342/ 579]              blk.28.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 343/ 579]            blk.28.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 344/ 579]                   blk.28.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 345/ 579]                 blk.28.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 346/ 579]                   blk.28.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 347/ 579]                 blk.28.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 348/ 579]               blk.28.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 349/ 579]               blk.28.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 350/ 579]               blk.28.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 351/ 579]                 blk.28.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 352/ 579]                   blk.29.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 353/ 579]                 blk.29.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 354/ 579]              blk.29.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 355/ 579]            blk.29.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 356/ 579]                   blk.29.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 357/ 579]                 blk.29.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 358/ 579]                   blk.29.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 359/ 579]                 blk.29.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 360/ 579]               blk.29.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 361/ 579]               blk.29.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 362/ 579]               blk.29.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 363/ 579]                 blk.29.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 364/ 579]                   blk.30.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 365/ 579]                 blk.30.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 366/ 579]              blk.30.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 367/ 579]            blk.30.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 368/ 579]                   blk.30.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 369/ 579]                 blk.30.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 370/ 579]                   blk.30.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 371/ 579]                 blk.30.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 372/ 579]               blk.30.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 373/ 579]               blk.30.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 374/ 579]               blk.30.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 375/ 579]                 blk.30.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 376/ 579]                   blk.31.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 377/ 579]                 blk.31.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 378/ 579]              blk.31.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 379/ 579]            blk.31.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 380/ 579]                   blk.31.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 381/ 579]                 blk.31.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 382/ 579]                   blk.31.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 383/ 579]                 blk.31.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 384/ 579]               blk.31.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 385/ 579]               blk.31.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 386/ 579]               blk.31.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 387/ 579]                 blk.31.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 388/ 579]                   blk.32.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 389/ 579]                 blk.32.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 390/ 579]              blk.32.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 391/ 579]            blk.32.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 392/ 579]                   blk.32.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 393/ 579]                 blk.32.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 394/ 579]                   blk.32.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 395/ 579]                 blk.32.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 396/ 579]               blk.32.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 397/ 579]               blk.32.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 398/ 579]               blk.32.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 399/ 579]                 blk.32.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 400/ 579]                   blk.33.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 401/ 579]                 blk.33.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 402/ 579]              blk.33.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 403/ 579]            blk.33.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 404/ 579]                   blk.33.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 405/ 579]                 blk.33.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 406/ 579]                   blk.33.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 407/ 579]                 blk.33.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 408/ 579]               blk.33.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 409/ 579]               blk.33.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 410/ 579]               blk.33.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 411/ 579]                 blk.33.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 412/ 579]                   blk.34.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 413/ 579]                 blk.34.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 414/ 579]              blk.34.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 415/ 579]            blk.34.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 416/ 579]                   blk.34.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 417/ 579]                 blk.34.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 418/ 579]                   blk.34.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 419/ 579]                 blk.34.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 420/ 579]               blk.34.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 421/ 579]               blk.34.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 422/ 579]               blk.34.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 423/ 579]                 blk.34.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 424/ 579]                   blk.35.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 425/ 579]                 blk.35.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 426/ 579]              blk.35.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 427/ 579]            blk.35.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 428/ 579]                   blk.35.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 429/ 579]                 blk.35.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 430/ 579]                   blk.35.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 431/ 579]                 blk.35.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 432/ 579]               blk.35.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 433/ 579]               blk.35.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 434/ 579]               blk.35.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 435/ 579]                 blk.35.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 436/ 579]                   blk.36.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 437/ 579]                 blk.36.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 438/ 579]              blk.36.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 439/ 579]            blk.36.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 440/ 579]                   blk.36.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 441/ 579]                 blk.36.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 442/ 579]                   blk.36.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 443/ 579]                 blk.36.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 444/ 579]               blk.36.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 445/ 579]               blk.36.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 446/ 579]               blk.36.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 447/ 579]                 blk.36.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 448/ 579]                   blk.37.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 449/ 579]                 blk.37.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 450/ 579]              blk.37.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 451/ 579]            blk.37.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 452/ 579]                   blk.37.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 453/ 579]                 blk.37.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 454/ 579]                   blk.37.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 455/ 579]                 blk.37.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 456/ 579]               blk.37.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 457/ 579]               blk.37.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 458/ 579]               blk.37.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 459/ 579]                 blk.37.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 460/ 579]                   blk.38.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 461/ 579]                 blk.38.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 462/ 579]              blk.38.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 463/ 579]            blk.38.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 464/ 579]                   blk.38.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 465/ 579]                 blk.38.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 466/ 579]                   blk.38.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 467/ 579]                 blk.38.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 468/ 579]               blk.38.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 469/ 579]               blk.38.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 470/ 579]               blk.38.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 471/ 579]                 blk.38.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 472/ 579]                   blk.39.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 473/ 579]                 blk.39.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 474/ 579]              blk.39.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 475/ 579]            blk.39.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 476/ 579]                   blk.39.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 477/ 579]                 blk.39.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 478/ 579]                   blk.39.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 479/ 579]                 blk.39.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 480/ 579]               blk.39.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 481/ 579]               blk.39.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 482/ 579]               blk.39.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 483/ 579]                 blk.39.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 484/ 579]                   blk.40.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 485/ 579]                 blk.40.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 486/ 579]              blk.40.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 487/ 579]            blk.40.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 488/ 579]                   blk.40.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 489/ 579]                 blk.40.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 490/ 579]                   blk.40.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 491/ 579]                 blk.40.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 492/ 579]               blk.40.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 493/ 579]               blk.40.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 494/ 579]               blk.40.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 495/ 579]                 blk.40.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 496/ 579]                   blk.41.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 497/ 579]                 blk.41.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 498/ 579]              blk.41.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 499/ 579]            blk.41.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 500/ 579]                   blk.41.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 501/ 579]                 blk.41.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 502/ 579]                   blk.41.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 503/ 579]                 blk.41.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 504/ 579]               blk.41.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 505/ 579]               blk.41.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 506/ 579]               blk.41.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 507/ 579]                 blk.41.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 508/ 579]                   blk.42.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 509/ 579]                 blk.42.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 510/ 579]              blk.42.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 511/ 579]            blk.42.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 512/ 579]                   blk.42.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 513/ 579]                 blk.42.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 514/ 579]                   blk.42.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 515/ 579]                 blk.42.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 516/ 579]               blk.42.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 517/ 579]               blk.42.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 518/ 579]               blk.42.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 519/ 579]                 blk.42.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 520/ 579]                   blk.43.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 521/ 579]                 blk.43.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 522/ 579]              blk.43.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 523/ 579]            blk.43.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 524/ 579]                   blk.43.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 525/ 579]                 blk.43.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 526/ 579]                   blk.43.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 527/ 579]                 blk.43.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 528/ 579]               blk.43.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 529/ 579]               blk.43.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 530/ 579]               blk.43.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 531/ 579]                 blk.43.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 532/ 579]                   blk.44.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 533/ 579]                 blk.44.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 534/ 579]              blk.44.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 535/ 579]            blk.44.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 536/ 579]                   blk.44.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 537/ 579]                 blk.44.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 538/ 579]                   blk.44.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 539/ 579]                 blk.44.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 540/ 579]               blk.44.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 541/ 579]               blk.44.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 542/ 579]               blk.44.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 543/ 579]                 blk.44.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 544/ 579]                   blk.45.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 545/ 579]                 blk.45.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 546/ 579]              blk.45.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 547/ 579]            blk.45.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 548/ 579]                   blk.45.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 549/ 579]                 blk.45.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 550/ 579]                   blk.45.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 551/ 579]                 blk.45.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 552/ 579]               blk.45.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 553/ 579]               blk.45.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 554/ 579]               blk.45.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 555/ 579]                 blk.45.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 556/ 579]                   blk.46.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 557/ 579]                 blk.46.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 558/ 579]              blk.46.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 559/ 579]            blk.46.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 560/ 579]                   blk.46.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 561/ 579]                 blk.46.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 562/ 579]                   blk.46.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 563/ 579]                 blk.46.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 564/ 579]               blk.46.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 565/ 579]               blk.46.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 566/ 579]               blk.46.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 567/ 579]                 blk.46.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 568/ 579]                   blk.47.attn_k.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 569/ 579]                 blk.47.attn_k.weight - [  5120,   1024,      1,      1], type =    f16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
      "[ 570/ 579]              blk.47.attn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 571/ 579]            blk.47.attn_output.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 572/ 579]                   blk.47.attn_q.bias - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 573/ 579]                 blk.47.attn_q.weight - [  5120,   5120,      1,      1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
      "[ 574/ 579]                   blk.47.attn_v.bias - [  1024,      1,      1,      1], type =    f32, size =    0.004 MiB\n",
      "[ 575/ 579]                 blk.47.attn_v.weight - [  5120,   1024,      1,      1], type =    f16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
      "[ 576/ 579]               blk.47.ffn_down.weight - [ 13824,   5120,      1,      1], type =    f16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
      "[ 577/ 579]               blk.47.ffn_gate.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "[ 578/ 579]               blk.47.ffn_norm.weight - [  5120,      1,      1,      1], type =    f32, size =    0.020 MiB\n",
      "[ 579/ 579]                 blk.47.ffn_up.weight - [  5120,  13824,      1,      1], type =    f16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
      "llama_model_quantize_impl: model size  = 28173.21 MiB (16.00 BPW)\n",
      "llama_model_quantize_impl: quant size  =  8566.04 MiB (4.87 BPW)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time = 212572.15 ms\n",
      "main:    total time = 212572.15 ms\n",
      "üéä ¬°√âXITO TOTAL! Descarga: /workspace/ollama-model-fine-tuning/MiPM_Senior_Final.gguf\n"
     ]
    }
   ],
   "source": [
    "# 1. LIMPIEZA RADICAL PARA LIBERAR ESPACIO DE CUOTA\n",
    "print(\"üßπ Limpiando rastros para recuperar cuota de sistema...\")\n",
    "!rm -rf llama.cpp/build\n",
    "!rm -rf /workspace/tmp/*\n",
    "!apt-get clean\n",
    "!rm -rf ~/.cache/pip\n",
    "\n",
    "# 2. CONFIGURACI√ìN DE RUTAS EN EL DISCO GRANDE (416TB)\n",
    "base_path = \"/workspace\"\n",
    "tmp_path = os.path.join(base_path, \"tmp\")\n",
    "os.makedirs(tmp_path, exist_ok=True)\n",
    "\n",
    "# Variables de entorno para forzar el uso de /workspace\n",
    "os.environ[\"TMPDIR\"] = tmp_path\n",
    "os.environ[\"TEMP\"] = tmp_path\n",
    "os.environ[\"TMP\"] = tmp_path\n",
    "# Esto le dice a CMake que no intente escribir en carpetas protegidas\n",
    "os.environ[\"CMAKE_CONFIG_DIR\"] = tmp_path \n",
    "\n",
    "def convert_and_quantize_ultimate_safe(model_path, output_final_gguf):\n",
    "    build_dir = os.path.join(base_path, \"llama_build_final\")\n",
    "    if os.path.exists(build_dir): shutil.rmtree(build_dir)\n",
    "    os.makedirs(build_dir)\n",
    "\n",
    "    temp_f16_gguf = os.path.join(base_path, \"model_temp_f16.gguf\")\n",
    "    \n",
    "    try:\n",
    "        # PASO 1: Conversi√≥n HF -> GGUF F16\n",
    "        print(\"‚öôÔ∏è PASO 1: Convirtiendo HF a GGUF F16...\")\n",
    "        subprocess.run([\n",
    "            \"poetry\", \"run\", \"python\", \"llama.cpp/convert_hf_to_gguf.py\",\n",
    "            model_path, \"--outfile\", temp_f16_gguf, \"--outtype\", \"f16\"\n",
    "        ], check=True)\n",
    "        \n",
    "        # PASO 2: Compilar usando prefijo local para evitar Disk Quota\n",
    "        print(\"üõ†Ô∏è PASO 2: Compilando localmente en /workspace...\")\n",
    "        # -DCMAKE_INSTALL_PREFIX: Evita que intente escribir en /usr/local\n",
    "        subprocess.run([\n",
    "            \"cmake\", \"-B\", build_dir, \"-S\", \"llama.cpp\", \n",
    "            \"-DGGML_CUDA=OFF\", \n",
    "            f\"-DCMAKE_INSTALL_PREFIX={build_dir}/install\"\n",
    "        ], check=True)\n",
    "        \n",
    "        subprocess.run([\n",
    "            \"cmake\", \"--build\", build_dir, \"--config\", \"Release\", \n",
    "            \"--target\", \"llama-quantize\", \"-j\"\n",
    "        ], check=True)\n",
    "        \n",
    "        # PASO 3: Cuantizaci√≥n\n",
    "        print(f\"üíé PASO 3: Cuantizando a q4_k_m...\")\n",
    "        quant_binary = os.path.join(build_dir, \"bin\", \"llama-quantize\")\n",
    "        # Si no est√° en bin, buscamos en la ra√≠z del build\n",
    "        if not os.path.exists(quant_binary):\n",
    "            quant_binary = os.path.join(build_dir, \"llama-quantize\")\n",
    "\n",
    "        subprocess.run([quant_binary, temp_f16_gguf, output_final_gguf, \"q4_k_m\"], check=True)\n",
    "        \n",
    "        if os.path.exists(temp_f16_gguf): os.remove(temp_f16_gguf)\n",
    "        print(f\"üéä ¬°√âXITO TOTAL! Descarga: {output_final_gguf}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cr√≠tico: {e}\")\n",
    "\n",
    "# --- EJECUCI√ìN ---\n",
    "convert_and_quantize_ultimate_safe(\"/workspace/ollama-model-fine-tuning/code/MiPM_Senior_HF\", \"/workspace/ollama-model-fine-tuning/MiPM_Senior_Final.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03cb66-929a-4f88-8fd0-46b07915f3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "per-training-model",
   "language": "python",
   "name": "per-training-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
