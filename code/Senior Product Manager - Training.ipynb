{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23c4f31-227b-4f9a-98d4-ba38f22ec8e1",
   "metadata": {},
   "source": [
    "### Carga del Modelo Base\n",
    "Vamos a traer a Qwen2.5-Coder-14B a la memoria de tu RTX 3090, comprimido en 4-bits para que entre sin problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9cbf0e7-39e5-4609-b165-6ffc520b9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       # Permite interactuar con el sistema operativo (rutas de carpetas, variables de entorno).\n",
    "import gc       # \"Garbage Collector\": Se usa para liberar memoria RAM/VRAM manualmente si es necesario.\n",
    "import json     # Para manipular archivos JSON (lectura de datasets o configuraciones).\n",
    "import torch    # La librer√≠a principal de PyTorch para operaciones con tensores y uso de la GPU.\n",
    "import shutil   # √ötil para operaciones de archivos de alto nivel, como borrar o mover carpetas completas.\n",
    "import subprocess # Permite ejecutar comandos de terminal (como git o pip) desde Python.\n",
    "\n",
    "# Importa la clase Dataset de Hugging Face para estructurar los datos de entrenamiento.\n",
    "from datasets import Dataset \n",
    "\n",
    "# Unsloth: Librer√≠a optimizada para entrenar modelos m√°s r√°pido y con menos memoria.\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "# Permite obtener informaci√≥n t√©cnica de un modelo alojado en el Hugging Face Hub.\n",
    "from huggingface_hub import model_info\n",
    "\n",
    "# Facilita la aplicaci√≥n de formatos de chat (como Llama-3 o Alpaca) a los datos.\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# El \"Entrenador\" (Trainer) especializado en Supervised Fine-Tuning (SFT).\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Define los hiperpar√°metros del entrenamiento (√©pocas, tasa de aprendizaje, pasos, etc.).\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158f1dca-a9c0-4393-9aff-e9dfd1754beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo encontrado en cach√©: /root/.cache/huggingface/hub/models--unsloth--Qwen2.5-Coder-14B-Instruct-bnb-4bit\n",
      "üîÑ Cargando modelo localmente...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 22.152 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:55<00:00, 27.51s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo cargado exitosamente en 4-bits.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuraci√≥n de par√°metros iniciales ---\n",
    "\n",
    "# Define la longitud m√°xima de tokens (contexto) que el modelo procesar√°. \n",
    "# 2048 es est√°ndar, pero Unsloth permite ampliarlo din√°micamente.\n",
    "max_seq_length = 2048 \n",
    "\n",
    "# El tipo de datos para los pesos (None deja que Unsloth lo detecte autom√°ticamente).\n",
    "# Usualmente detectar√° float16 o bfloat16 seg√∫n tu GPU.\n",
    "dtype = None \n",
    "\n",
    "# Activa la cuantizaci√≥n de 4 bits. Crucial para que un modelo de 14B \n",
    "# quepa en GPUs de consumo (como una RTX 3060/4060 o superiores).\n",
    "load_in_4bit = True \n",
    "\n",
    "# El identificador del modelo en Hugging Face. \n",
    "# Esta versi√≥n ya viene pre-cuantizada (\"bnb-4bit\") para ser ultra r√°pida.\n",
    "model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\"\n",
    "\n",
    "# --- Verificaci√≥n de archivos locales (Cach√©) ---\n",
    "\n",
    "# Construye la ruta donde Hugging Face suele guardar los modelos descargados.\n",
    "# Transforma \"usuario/modelo\" en el formato de carpetas de cach√© del sistema.\n",
    "cache_dir = os.path.expanduser(f\"~/.cache/huggingface/hub/models--{'--'.join(model_name.split('/'))}\")\n",
    "\n",
    "# Comprueba si la carpeta del modelo ya existe en el disco duro.\n",
    "if os.path.exists(cache_dir):\n",
    "    # Si existe, nos avisa que no gastar√° internet descarg√°ndolo de nuevo.\n",
    "    print(f\"‚úÖ Modelo encontrado en cach√©: {cache_dir}\")\n",
    "    print(\"üîÑ Cargando modelo localmente...\")\n",
    "else:\n",
    "    # Si no existe, nos advierte que iniciar√° una descarga pesada.\n",
    "    print(f\"‚ö†Ô∏è Modelo NO encontrado en cach√©. Se descargar√° en: {cache_dir}\")\n",
    "\n",
    "# --- Carga del Modelo y el Tokenizador ---\n",
    "\n",
    "# Utiliza la funci√≥n optimizada de Unsloth para cargar el modelo en la VRAM de la GPU.\n",
    "# Retorna dos objetos: \n",
    "# 1. model: El cerebro del IA.\n",
    "# 2. tokenizer: El traductor que convierte texto en n√∫meros (tokens).\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# Confirmaci√≥n final de que el modelo est√° listo para usarse o entrenarse.\n",
    "print(\"‚úÖ Modelo cargado exitosamente en 4-bits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c773f-4058-4538-9ff8-d6ae84ee1a79",
   "metadata": {},
   "source": [
    "### Verificar que el modelo existe manualmente\n",
    "Si ninguna de las anteriores funciona, verifica que puedes acceder al modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0710312-e904-44c1-864b-b511e399badf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo encontrado: unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\n",
      "üì• Descargas: 8706\n"
     ]
    }
   ],
   "source": [
    "# --- Verificaci√≥n de metadatos en Hugging Face Hub ---\n",
    "\n",
    "# Usamos un bloque \"try-except\" para manejar posibles errores de conexi√≥n o permisos.\n",
    "try:\n",
    "    # Llama a la API de Hugging Face para obtener la ficha t√©cnica (info) del modelo.\n",
    "    # Esto no descarga el modelo, solo consulta sus estad√≠sticas y estado.\n",
    "    info = model_info(\"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\")\n",
    "    \n",
    "    # Si la consulta es exitosa, imprime el ID confirmado del modelo.\n",
    "    print(f\"‚úÖ Modelo encontrado: {info.id}\")\n",
    "    \n",
    "    # Muestra el n√∫mero total de descargas que ha tenido el modelo (popularidad).\n",
    "    print(f\"üì• Descargas: {info.downloads}\")\n",
    "\n",
    "# Si ocurre un error (ej. no hay internet, el modelo es privado o el nombre est√° mal escrito):\n",
    "except Exception as e:\n",
    "    # Atrapa el error y lo muestra en pantalla sin detener la ejecuci√≥n del programa.\n",
    "    print(f\"‚ùå Error accediendo al modelo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827c010-58fb-427a-8181-32708028502e",
   "metadata": {},
   "source": [
    "### Inyecci√≥n de Adaptadores (LoRA)\n",
    "Aqu√≠ definimos la arquitectura del fine-tuning. Solo vamos a entrenar una fracci√≥n del modelo (los adaptadores), lo que hace que el proceso sea r√°pido y eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff301f5-a741-40d4-a4b5-fa806bbf9150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.2.1 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Adaptadores LoRA configurados.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuraci√≥n de PEFT (Parameter-Efficient Fine-Tuning) con LoRA ---\n",
    "\n",
    "# Transforma el modelo base en un modelo PEFT (solo una parte es entrenable).\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    # 'r' (Rank): Define el tama√±o de las matrices de bajo rango. \n",
    "    # 16 es un equilibrio ideal entre precisi√≥n y ahorro de memoria.\n",
    "    r = 16, \n",
    "\n",
    "    # 'target_modules': Especifica en qu√© capas del modelo se inyectar√°n los adaptadores.\n",
    "    # Estas capas (q, k, v, o, gate, up, down) cubren casi toda la atenci√≥n y redes neuronales del modelo.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "\n",
    "    # 'lora_alpha': Escala el aprendizaje de los adaptadores. \n",
    "    # Generalmente se recomienda que sea igual o el doble de 'r'.\n",
    "    lora_alpha = 16,\n",
    "\n",
    "    # 'lora_dropout': Probabilidad de desactivar neuronas al azar para evitar sobreajuste.\n",
    "    # 0 es lo m√°s eficiente para velocidad de entrenamiento en Unsloth.\n",
    "    lora_dropout = 0, \n",
    "\n",
    "    # 'bias': Define si se entrenan los sesgos. \"none\" es lo est√°ndar para LoRA.\n",
    "    bias = \"none\",\n",
    "\n",
    "    # 'use_gradient_checkpointing': T√©cnica que libera memoria RAM de la GPU \n",
    "    # guardando solo lo esencial. \"unsloth\" usa una versi√≥n optimizada que gasta un 30% menos.\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "\n",
    "    # Semilla aleatoria para que los resultados sean reproducibles (siempre den lo mismo).\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# Mensaje de confirmaci√≥n: el modelo ahora est√° listo para recibir datos de entrenamiento.\n",
    "print(\"‚úÖ Adaptadores LoRA configurados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c7aeb-6101-4a9a-87e1-61c9692e44ed",
   "metadata": {},
   "source": [
    "#### Preparaci√≥n del Dataset\n",
    "Le ense√±amos al modelo a entender el formato de tu dataset.jsonl (ChatML: System, User, Assistant). Asumimos que dentro de tu JSONL, el arreglo de mensajes se llama messages o conversations.\n",
    "\n",
    "Este c√≥digo es fundamental: act√∫a como el traductor entre la forma en que t√∫ guardaste la informaci√≥n en tu archivo `epics.jsonl` y la forma exacta en la que el modelo (Qwen2.5) necesita leerla para aprender.\n",
    "\n",
    "En t√©rminos sencillos, el modelo no entiende \"columnas\" o \"diccionarios\"; solo entiende secuencias largas de texto con etiquetas especiales que le indican qui√©n est√° hablando.\n",
    "\n",
    "#### 1. Aplicar la plantilla ChatML (`get_chat_template`)\n",
    "\n",
    "Los modelos conversacionales como Qwen usan un formato llamado **ChatML** (Chat Markup Language). Esto significa que usan \"etiquetas invisibles\" para separar los mensajes, como `<|im_start|>user` y `<|im_end|>`.\n",
    "Este paso configura tu tokenizador para que inyecte autom√°ticamente estas etiquetas en el texto, ahorr√°ndote el trabajo de escribirlas a mano.\n",
    "\n",
    "#### 2. Definir el System Prompt\n",
    "\n",
    "Aqu√≠ le inyectamos la personalidad a tu PM Senior. Le estamos diciendo expl√≠citamente cu√°l es su rol y, muy importante, le indicamos que **debe responder en formato JSON**. Esto ancla el comportamiento del modelo para que siempre act√∫e como un experto estructurado.\n",
    "\n",
    "#### 3. La funci√≥n de formateo (`formatting_prompts_func`)\n",
    "\n",
    "Este es el \"motor\" de la celda. Como tu archivo tiene una columna llamada `input` y otra llamada `output`, la funci√≥n hace lo siguiente para cada fila de tu dataset:\n",
    "\n",
    "* **Extrae los datos:** Toma el diccionario crudo de `input` y el de `output`.\n",
    "* **Formatea a texto JSON legible (`json.dumps`):** Este paso es el truco vital. Como tu objetivo es que el modelo devuelva un JSON perfecto (como se ve en tus salidas con `epic_id`, `title`, `acceptance_criteria`), usamos `json.dumps(..., indent=2)` para transformar tus diccionarios en cadenas de texto con saltos de l√≠nea y tabulaciones perfectas. As√≠ el modelo aprende a indentar como un humano.\n",
    "* **Arma la conversaci√≥n:** Crea una lista l√≥gica con tres roles:\n",
    "1. El `system` (las instrucciones base).\n",
    "2. El `user` (tu `input` con el contexto y requerimientos).\n",
    "3. El `assistant` (tu `output` con la Epic dorada).\n",
    "\n",
    "\n",
    "* **Aplica la plantilla (`apply_chat_template`):** Pasa esa lista estructurada por el tokenizador para fusionarla en un √∫nico bloque de texto continuo con todas las etiquetas ChatML listas para el entrenamiento.\n",
    "\n",
    "#### 4. Cargar y procesar (`load_dataset` y `map`)\n",
    "\n",
    "* Usa la librer√≠a de Hugging Face para cargar tu archivo `epics.jsonl` a la memoria RAM de tu RunPod.\n",
    "* El comando `.map(..., batched=True)` pasa todo tu dataset por la funci√≥n que explicamos arriba de forma simult√°nea y s√∫per r√°pida.\n",
    "* El resultado final es que tu dataset ahora tiene una nueva columna llamada **`text`**, que contiene la conversaci√≥n perfectamente formateada. Esta columna `text` es la *√∫nica* que Qwen va a leer durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13f2245-b713-4332-aa75-511e7cce324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°Dataset cargado y formateado! Total de ejemplos procesados: 104\n",
      "\n",
      "--- MUESTRA DEL PRIMER EJEMPLO FORMATEADO ---\n",
      "<|im_start|>system\n",
      "Eres un Product Manager Senior experto. Tu tarea es analizar el contexto y los requerimientos proporcionados para redactar Epics de software detalladas, estructuradas y precisas en formato JSON.<|im_end|>\n",
      "<|im_start|>user\n",
      "{\n",
      "  \"context\": \"El proyecto inicia desde cero, sin ning√∫n tipo de infraestructura en la nube. Es imperativo establecer una base s√≥lida, repetible y segura que permita el despliegue y la operaci√≥n de todos los componentes subsecuentes de la plataforma. La adop...\n",
      "[CONTIN√öA]\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Preparaci√≥n del Formato de Conversaci√≥n ---\n",
    "\n",
    "# Configura el tokenizador para que use la estructura \"ChatML\" (<|im_start|>, <|im_end|>).\n",
    "# Esto es vital para que el modelo sepa cu√°ndo termina de hablar el usuario y empieza √©l.\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\",\n",
    ")\n",
    "\n",
    "# --- 2. Definici√≥n de la Identidad (System Prompt) ---\n",
    "\n",
    "# Establecemos las \"instrucciones de comportamiento\" del modelo. \n",
    "# Aqu√≠ le decimos que sea un Product Manager experto y que responda en JSON.\n",
    "system_prompt = \"Eres un Product Manager Senior experto. Tu tarea es analizar el contexto y los requerimientos proporcionados para redactar Epics de software detalladas, estructuradas y precisas en formato JSON.\"\n",
    "\n",
    "formatted_texts = []\n",
    "\n",
    "# --- 3. Procesamiento Manual del Archivo de Datos ---\n",
    "\n",
    "# Ruta donde tienes guardados tus ejemplos de entrenamiento (formato JSON Lines).\n",
    "file_path = \"../data/epics.jsonl\" \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        # Si la l√≠nea est√° vac√≠a, nos la saltamos para evitar errores.\n",
    "        if not line.strip(): continue \n",
    "        \n",
    "        # Convertimos la l√≠nea de texto (JSON) en un diccionario de Python.\n",
    "        record = json.loads(line)\n",
    "        \n",
    "        # Convertimos los campos 'input' y 'output' en texto (strings) bien formateados.\n",
    "        # 'ensure_ascii=False' permite tildes y √±; 'indent=2' lo hace legible.\n",
    "        user_content = json.dumps(record[\"input\"], ensure_ascii=False, indent=2)\n",
    "        assistant_content = json.dumps(record[\"output\"], ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Creamos la estructura de la conversaci√≥n (Mensaje de Sistema -> Usuario -> Asistente).\n",
    "        convo = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        \n",
    "        # 'apply_chat_template' une todo lo anterior usando las etiquetas especiales de ChatML.\n",
    "        # tokenize=False: Solo genera el texto plano por ahora.\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        \n",
    "        # Guardamos el resultado en una lista de diccionarios con la clave \"text\".\n",
    "        formatted_texts.append({\"text\": text})\n",
    "\n",
    "# --- 4. Creaci√≥n del Dataset Final ---\n",
    "\n",
    "# Convertimos nuestra lista de Python en un objeto Dataset de HuggingFace.\n",
    "# Este formato es el que el 'SFTTrainer' de Unsloth requiere para empezar a entrenar.\n",
    "dataset = Dataset.from_list(formatted_texts)\n",
    "\n",
    "# Mensajes de control para verificar que todo sali√≥ bien.\n",
    "print(f\"‚úÖ ¬°Dataset cargado y formateado! Total de ejemplos procesados: {len(dataset)}\")\n",
    "\n",
    "# Imprimimos los primeros 500 caracteres del primer ejemplo para ver las etiquetas <|im_start|>.\n",
    "print(\"\\n--- MUESTRA DEL PRIMER EJEMPLO FORMATEADO ---\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\\n[CONTIN√öA]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a428ef1-0453-4c79-8a4a-cefd7378a9fe",
   "metadata": {},
   "source": [
    "### **Fine-Tunning** (ajusta) un modelo de lenguaje pre-entrenado con tus datos espec√≠ficos usando **LoRA**\n",
    "\n",
    "| Componente | Funci√≥n |\n",
    "|------------|---------|\n",
    "| `SFTTrainer` | Entrenador especializado para \"Supervised Fine-Tuning\" (ajuste supervisado) |\n",
    "| `train_dataset` | Tus datos de entrenamiento (104 ejemplos conversacionales) |\n",
    "| `max_seq_length` | L√≠mite de tokens por ejemplo (2048) |\n",
    "| `packing=False` | Respeta la estructura conversacional exacta de cada ejemplo |\n",
    "| `per_device_train_batch_size=2` | Procesa 2 ejemplos simult√°neamente en GPU |\n",
    "| `gradient_accumulation_steps=4` | Simula un lote de 8 ejemplos (2√ó4) para ahorrar VRAM |\n",
    "| `num_train_epochs=3` | Pasa 3 veces por todo el dataset (~39 pasos totales) |\n",
    "| `learning_rate=2e-4` | Velocidad de aprendizaje est√°ndar para LoRA |\n",
    "| `adamw_8bit` | Optimizador comprimido que usa menos memoria |\n",
    "| `fp16/bf16` | Precisi√≥n mixta para acelerar entrenamiento |\n",
    "\n",
    "**Tiempo estimado:** ~39 pasos √ó tiempo por paso (var√≠a seg√∫n GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01333232-cdf6-408e-a226-05feefb79916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Cargando modelo base...\n",
      "==((====))==  Unsloth 2026.2.1: Fast Qwen2 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 22.152 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Inyectando adaptadores LoRA (Modo Ligero)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2026.2.1 patched 48 layers with 48 QKV layers, 48 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Formateando el dataset...\n",
      "üöÄ ¬°INICIANDO FINE-TUNING EXTREMO!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=64): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104/104 [00:13<00:00,  7.70 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 104 | Num Epochs = 3 | Total steps = 39\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 12,582,912 of 14,782,616,576 (0.09% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 02:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.722500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.813900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.603400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.371400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°Entrenamiento completado en 165.8221 segundos!\n"
     ]
    }
   ],
   "source": [
    "# --- 0. LIMPIEZA PROFUNDA DE GPU ---\n",
    "# Libera cualquier residuo de memoria en la GPU para empezar desde cero.\n",
    "torch.cuda.empty_cache() \n",
    "# Fuerza al recolector de basura de Python a limpiar objetos no utilizados en la RAM.\n",
    "gc.collect() \n",
    "\n",
    "# --- 1. CARGA DEL MODELO ---\n",
    "# Aumentamos la longitud de secuencia a 4096 para manejar Epics m√°s largas.\n",
    "max_seq_length = 4096 \n",
    "print(\"‚è≥ Cargando modelo base...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Cuantizaci√≥n para reducir el peso del modelo en VRAM.\n",
    ")\n",
    "\n",
    "# --- 2. ADAPTADORES LORA (MODO LIGERO) ---\n",
    "print(\"üß† Inyectando adaptadores LoRA (Modo Ligero)...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Reducimos el rango (Rank). Menos par√°metros entrenables = menos memoria.\n",
    "    # Solo entrenamos las capas de atenci√≥n (proyecciones Q, K, V, O).\n",
    "    # Al quitar \"gate_proj\", \"up_proj\" y \"down_proj\", ahorramos mucha VRAM.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# --- 3. PREPARAR DATASET ---\n",
    "# (Este bloque aplica la plantilla ChatML y limpia el JSONL como vimos anteriormente)\n",
    "print(\"üìä Formateando el dataset...\")\n",
    "tokenizer = get_chat_template(tokenizer, chat_template = \"chatml\")\n",
    "system_prompt = \"Eres un Product Manager Senior experto...\"\n",
    "formatted_texts = []\n",
    "\n",
    "with open(\"../data/epics.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip(): continue\n",
    "        record = json.loads(line)\n",
    "        user_content = json.dumps(record[\"input\"], ensure_ascii=False, indent=2)\n",
    "        assistant_content = json.dumps(record[\"output\"], ensure_ascii=False, indent=2)\n",
    "        convo = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        formatted_texts.append({\"text\": text})\n",
    "\n",
    "dataset = Dataset.from_list(formatted_texts)\n",
    "\n",
    "# --- 4. ENTRENAMIENTO BLINDADO CONTRA OOM ---\n",
    "print(\"üöÄ ¬°INICIANDO FINE-TUNING EXTREMO!\")\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2, # Usa 2 procesos para cargar datos m√°s r√°pido.\n",
    "    packing = False,      # No empaqueta secuencias cortas (m√°s lento pero m√°s estable).\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1, # Procesa 1 ejemplo a la vez para no saturar la GPU.\n",
    "        gradient_accumulation_steps = 8, # Actualiza pesos cada 8 pasos (Batch efectivo = 8).\n",
    "        warmup_steps = 5,                # Sube la intensidad del aprendizaje gradualmente.\n",
    "        num_train_epochs = 3,            # El modelo ver√° el dataset completo 3 veces.\n",
    "        learning_rate = 2e-4,            # Velocidad de aprendizaje (est√°ndar para LoRA).\n",
    "        # Selecciona autom√°ticamente entre fp16 o bf16 seg√∫n la potencia de tu GPU.\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,               # Muestra el progreso en cada paso.\n",
    "        # \"paged_adamw_8bit\": Si la GPU se queda sin memoria, usa la RAM del sistema como \"colch√≥n\".\n",
    "        optim = \"paged_adamw_8bit\", \n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",          # Carpeta donde se guardar√°n los checkpoints.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Inicia el proceso de entrenamiento y guarda las estad√≠sticas finales.\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"‚úÖ ¬°Entrenamiento completado en {trainer_stats.metrics['train_runtime']} segundos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843da0b1-bc9e-4765-9fea-282216b45da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease   \n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Fetched 384 kB in 1s (433 kB/s)                                   \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "build-essential set to manually installed.\n",
      "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
      "libcurl4-openssl-dev is already the newest version (7.81.0-1ubuntu1.21).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 134 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# 'apt-get update -y': Actualiza la lista de paquetes disponibles en los repositorios.\n",
    "# El flag '-y' responde autom√°ticamente \"s√≠\" a las confirmaciones para que no se detenga.\n",
    "!apt-get update -y && \\\n",
    "\n",
    "# 'apt-get install -y': Comando para instalar nuevos paquetes.\n",
    "# Se instalan 3 componentes fundamentales para compilar software:\n",
    "# 1. cmake: Herramienta avanzada para gestionar el proceso de compilaci√≥n (indispensable para muchas librer√≠as de IA).\n",
    "# 2. build-essential: Un paquete que incluye el compilador GCC, G++ y herramientas b√°sicas para crear software desde el c√≥digo fuente.\n",
    "# 3. libcurl4-openssl-dev: Librer√≠a necesaria para que las aplicaciones puedan realizar transferencias de red (como descargar modelos o comunicarse con APIs) de forma segura.\n",
    "apt-get install -y cmake build-essential libcurl4-openssl-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3b5cc1-12e6-4e7d-bd10-919692dbcab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  - \u001b[36mgguf\u001b[39m\n",
      "  - \u001b[36mprotobuf\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Using version \u001b[39;1m^0.2.1\u001b[39;22m for \u001b[36msentencepiece\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(0.6s)\u001b[39;22m\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# '!': Ejecuta el comando en la consola del sistema.\n",
    "# 'poetry add': Instala las librer√≠as y las registra en tu archivo de proyecto.\n",
    "\n",
    "!poetry add \\\n",
    "    # 'gguf': Necesario para escribir y leer el formato de archivo final (.gguf).\n",
    "    gguf \\\n",
    "    \n",
    "    # 'protobuf': El sistema de serializaci√≥n de datos que usa Google y Hugging Face.\n",
    "    protobuf \\\n",
    "    \n",
    "    # 'sentencepiece': ¬°La pieza clave! Es la librer√≠a que maneja la tokenizaci√≥n \n",
    "    # de modelos como Qwen, Llama y Mistral. Sin esto, el modelo no puede \n",
    "    # descomponer las palabras en unidades que entienda (tokens).\n",
    "    sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22105179-cc95-4f07-a87b-7524542e690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '!': Indica que el comando se ejecuta en la consola del sistema (Shell).\n",
    "# 'rm': Es el comando para \"remover\" (borrar) archivos o directorios.\n",
    "\n",
    "!rm -rf \\\n",
    "    # '-r' (recursive): Permite borrar carpetas y todo su contenido interno (subcarpetas y archivos).\n",
    "    # '-f' (force): Fuerza el borrado ignorando archivos inexistentes y sin pedir confirmaci√≥n al usuario.\n",
    "    -rf \\\n",
    "    \n",
    "    # 'llama.cpp': El nombre de la carpeta espec√≠fica que se desea eliminar.\n",
    "    llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "700780ac-532f-44f6-a310-b8d74d04a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ llama.cpp ya existe\n",
      "üì¶ Iniciando fusi√≥n y exportaci√≥n a GGUF...\n",
      "‚è≥ Esto puede tomar 10-20 minutos...\n",
      "‚ö†Ô∏è Error con m√©todo autom√°tico: unsloth_save_pretrained_gguf() got an unexpected keyword argument 'converter_location'\n",
      "üîÑ Intentando m√©todo manual...\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00006.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:28<00:00, 24.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [01:10<00:00, 11.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/code/MiPM_Senior_HF`\n",
      "‚úÖ Modelo guardado en formato HF\n",
      "üìù Para convertir a GGUF manualmente, ejecuta en terminal:\n",
      "\n",
      "    cd ./llama.cpp\n",
      "    python convert_hf_to_gguf.py ../MiPM_Senior_HF --outfile ../MiPM_Senior.gguf --outtype q4_k_m\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# --- Verificaci√≥n y Setup de llama.cpp ---\n",
    "\n",
    "# Definimos la ruta local donde queremos que viva el repositorio llama.cpp.\n",
    "LLAMA_CPP_DIR = \"./llama.cpp\"\n",
    "\n",
    "# Comprobamos si la carpeta ya existe para no descargarla dos veces.\n",
    "if not os.path.exists(LLAMA_CPP_DIR):\n",
    "    print(\"üì• Descargando llama.cpp...\")\n",
    "    # 'git clone': Descarga el c√≥digo fuente oficial de llama.cpp.\n",
    "    subprocess.run([\n",
    "        \"git\", \"clone\", \n",
    "        \"https://github.com/ggerganov/llama.cpp.git\",\n",
    "        LLAMA_CPP_DIR\n",
    "    ], check=True)\n",
    "    print(\"‚úÖ llama.cpp descargado\")\n",
    "    \n",
    "    # Instalamos las librer√≠as de Python necesarias para que los scripts de conversi√≥n funcionen.\n",
    "    print(\"üì¶ Instalando dependencias...\")\n",
    "    subprocess.run([\n",
    "        \"pip\", \"install\", \"-r\", \n",
    "        f\"{LLAMA_CPP_DIR}/requirements.txt\"\n",
    "    ], check=True)\n",
    "else:\n",
    "    # Si la carpeta ya est√° ah√≠, simplemente lo confirmamos.\n",
    "    print(\"‚úÖ llama.cpp ya existe\")\n",
    "\n",
    "# --- Proceso de Exportaci√≥n ---\n",
    "\n",
    "print(\"üì¶ Iniciando fusi√≥n y exportaci√≥n a GGUF...\")\n",
    "print(\"‚è≥ Esto puede tomar 10-20 minutos...\")\n",
    "\n",
    "try:\n",
    "    # Intento A: Usar la funci√≥n integrada de Unsloth.\n",
    "    model.save_pretrained_gguf(\n",
    "        \"MiPM_Senior\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_m\", # Cuantizaci√≥n de 4 bits (calidad media-alta).\n",
    "        # Indicamos expl√≠citamente d√≥nde est√° la herramienta de conversi√≥n.\n",
    "        converter_location=LLAMA_CPP_DIR,\n",
    "    )\n",
    "    print(\"‚úÖ ¬°PROCESO COMPLETADO!\")\n",
    "    print(\"üìÅ Archivo generado: MiPM_Senior_q4_k_m.gguf\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Intento B (Fallback): Si lo anterior falla (por errores de memoria o librer√≠as),\n",
    "    # hacemos el proceso en dos pasos manuales.\n",
    "    print(f\"‚ö†Ô∏è Error con m√©todo autom√°tico: {e}\")\n",
    "    print(\"üîÑ Intentando m√©todo manual...\")\n",
    "    \n",
    "    # 1. Primero, fusionamos el modelo LoRA con el base y lo guardamos como un modelo normal de Hugging Face.\n",
    "    # 'merged_16bit' asegura que no haya p√©rdida de calidad en esta fase.\n",
    "    model.save_pretrained_merged(\n",
    "        \"MiPM_Senior_HF\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\",\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Modelo guardado en formato HF\")\n",
    "    # 2. Instrucciones para que el usuario ejecute la conversi√≥n final desde la consola.\n",
    "    print(\"üìù Para convertir a GGUF manualmente, ejecuta en terminal:\")\n",
    "    print(f\"\"\"\n",
    "    cd {LLAMA_CPP_DIR}\n",
    "    python convert_hf_to_gguf.py ../MiPM_Senior_HF --outfile ../MiPM_Senior.gguf --outtype q4_k_m\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "239a5721-0d86-4640-83fb-25346d729775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease   \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
      "g++ set to manually installed.\n",
      "gcc is already the newest version (4:11.2.0-1ubuntu1).\n",
      "gcc set to manually installed.\n",
      "make is already the newest version (4.3-4.1build1).\n",
      "make set to manually installed.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 134 not upgraded.\n",
      "\n",
      "--- ESPACIO EN DISCO ---\n",
      "Filesystem                    Size  Used Avail Use% Mounted on\n",
      "mfs#eur-no-1.runpod.net:9421 1006T  591T  416T  59% /workspace\n"
     ]
    }
   ],
   "source": [
    "# Instalar herramientas de compilaci√≥n esenciales\n",
    "!apt-get update && apt-get install -y build-essential gcc g++ make\n",
    "\n",
    "# Verificar espacio en disco en /workspace\n",
    "print(\"\\n--- ESPACIO EN DISCO ---\")\n",
    "!df -h /workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0afc2c69-dcae-4183-b41e-150b9821eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /workspace/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a60f8fe-aea8-4761-bb57-9479e323b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Limpiando rastros para recuperar cuota de sistema...\n",
      "‚öôÔ∏è PASO 1: Convirtiendo HF a GGUF F16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: MiPM_Senior_HF\n",
      "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00005-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00006-of-00006.safetensors'\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.36.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.36.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.36.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.37.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.37.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.37.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.38.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.38.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.38.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.39.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.39.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.39.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.40.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.40.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.40.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.41.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.41.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.41.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> F16, shape = {5120, 152064}\n",
      "INFO:hf-to-gguf:blk.42.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.42.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.42.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.42.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.43.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.43.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.43.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.44.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.44.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.44.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.45.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.45.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.45.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.46.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.46.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.46.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_down.weight,    torch.bfloat16 --> F16, shape = {13824, 5120}\n",
      "INFO:hf-to-gguf:blk.47.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.47.ffn_up.weight,      torch.bfloat16 --> F16, shape = {5120, 13824}\n",
      "INFO:hf-to-gguf:blk.47.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_output.weight, torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.weight,      torch.bfloat16 --> F16, shape = {5120, 5120}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.weight,      torch.bfloat16 --> F16, shape = {5120, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {5120}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 13824\n",
      "INFO:hf-to-gguf:gguf: head count = 40\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "The tokenizer you are loading from '/workspace/code/MiPM_Senior_HF' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151665\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/workspace/model_temp_f16.gguf: n_tensors = 579, total_size = 29.5G\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5G/29.5G [02:38<00:00, 186Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /workspace/model_temp_f16.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è PASO 2: Compilando localmente en /workspace...\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The ASM compiler identification is GNU\n",
      "-- Found assembler: /usr/bin/cc\n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- ggml version: 0.9.7\n",
      "-- ggml commit:  10b26ee23\n",
      "-- Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the system variable OPENSSL_ROOT_DIR (missing: OPENSSL_CRYPTO_LIBRARY OPENSSL_INCLUDE_DIR) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mCMake Warning at vendor/cpp-httplib/CMakeLists.txt:150 (message):\n",
      "  OpenSSL not found, HTTPS support disabled\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Generating embedded license file for target: common\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /workspace/llama_build_final\n",
      "[  0%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  4%] Built target build_info\n",
      "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  4%] Built target ggml-base\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[ 12%] Built target ggml-cpu\n",
      "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
      "[ 14%] Built target cpp-httplib\n",
      "[ 14%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "[ 14%] Built target ggml\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampler.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/delta-net-base.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais2.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba-base.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/paddleocr.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen35.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "[ 82%] Built target llama\n",
      "[ 82%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/debug.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-map.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-mod.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/parser.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/lexer.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/value.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/string.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/runtime.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/caps.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/__/license.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 97%] Built target common\n",
      "[ 97%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "[100%] Built target llama-quantize\n",
      "üíé PASO 3: Cuantizando a q4_k_m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 8114 (10b26ee23)\n",
      "main: built with GNU 11.4.0 for Linux x86_64\n",
      "main: quantizing '/workspace/model_temp_f16.gguf' to '/workspace/MiPM_Senior_Final.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 579 tensors from /workspace/model_temp_f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = MiPM_Senior_HF\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 15B\n",
      "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 48\n",
      "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151665\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - type  f32:  241 tensors\n",
      "llama_model_loader: - type  f16:  338 tensors\n",
      "[   1/ 579]                        output.weight - [ 5120, 152064,     1,     1], type =    f16, converting to q6_K .. size =  1485.00 MiB ->   609.08 MiB\n",
      "[   2/ 579]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error cr√≠tico: Command '['/workspace/llama_build_final/bin/llama-quantize', '/workspace/model_temp_f16.gguf', '/workspace/MiPM_Senior_Final.gguf', 'q4_k_m']' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_quantize: failed to quantize: basic_ios::clear: iostream error\n",
      "main: failed to quantize model from '/workspace/model_temp_f16.gguf'\n"
     ]
    }
   ],
   "source": [
    "# 1. LIMPIEZA RADICAL PARA LIBERAR ESPACIO DE CUOTA\n",
    "print(\"üßπ Limpiando rastros para recuperar cuota de sistema...\")\n",
    "!rm -rf llama.cpp/build\n",
    "!rm -rf /workspace/tmp/*\n",
    "!apt-get clean\n",
    "!rm -rf ~/.cache/pip\n",
    "\n",
    "# 2. CONFIGURACI√ìN DE RUTAS EN EL DISCO GRANDE (416TB)\n",
    "base_path = \"/workspace\"\n",
    "tmp_path = os.path.join(base_path, \"tmp\")\n",
    "os.makedirs(tmp_path, exist_ok=True)\n",
    "\n",
    "# Variables de entorno para forzar el uso de /workspace\n",
    "os.environ[\"TMPDIR\"] = tmp_path\n",
    "os.environ[\"TEMP\"] = tmp_path\n",
    "os.environ[\"TMP\"] = tmp_path\n",
    "# Esto le dice a CMake que no intente escribir en carpetas protegidas\n",
    "os.environ[\"CMAKE_CONFIG_DIR\"] = tmp_path \n",
    "\n",
    "def convert_and_quantize_ultimate_safe(model_path, output_final_gguf):\n",
    "    build_dir = os.path.join(base_path, \"llama_build_final\")\n",
    "    if os.path.exists(build_dir): shutil.rmtree(build_dir)\n",
    "    os.makedirs(build_dir)\n",
    "\n",
    "    temp_f16_gguf = os.path.join(base_path, \"model_temp_f16.gguf\")\n",
    "    \n",
    "    try:\n",
    "        # PASO 1: Conversi√≥n HF -> GGUF F16\n",
    "        print(\"‚öôÔ∏è PASO 1: Convirtiendo HF a GGUF F16...\")\n",
    "        subprocess.run([\n",
    "            \"poetry\", \"run\", \"python\", \"llama.cpp/convert_hf_to_gguf.py\",\n",
    "            model_path, \"--outfile\", temp_f16_gguf, \"--outtype\", \"f16\"\n",
    "        ], check=True)\n",
    "        \n",
    "        # PASO 2: Compilar usando prefijo local para evitar Disk Quota\n",
    "        print(\"üõ†Ô∏è PASO 2: Compilando localmente en /workspace...\")\n",
    "        # -DCMAKE_INSTALL_PREFIX: Evita que intente escribir en /usr/local\n",
    "        subprocess.run([\n",
    "            \"cmake\", \"-B\", build_dir, \"-S\", \"llama.cpp\", \n",
    "            \"-DGGML_CUDA=OFF\", \n",
    "            f\"-DCMAKE_INSTALL_PREFIX={build_dir}/install\"\n",
    "        ], check=True)\n",
    "        \n",
    "        subprocess.run([\n",
    "            \"cmake\", \"--build\", build_dir, \"--config\", \"Release\", \n",
    "            \"--target\", \"llama-quantize\", \"-j\"\n",
    "        ], check=True)\n",
    "        \n",
    "        # PASO 3: Cuantizaci√≥n\n",
    "        print(f\"üíé PASO 3: Cuantizando a q4_k_m...\")\n",
    "        quant_binary = os.path.join(build_dir, \"bin\", \"llama-quantize\")\n",
    "        # Si no est√° en bin, buscamos en la ra√≠z del build\n",
    "        if not os.path.exists(quant_binary):\n",
    "            quant_binary = os.path.join(build_dir, \"llama-quantize\")\n",
    "\n",
    "        subprocess.run([quant_binary, temp_f16_gguf, output_final_gguf, \"q4_k_m\"], check=True)\n",
    "        \n",
    "        if os.path.exists(temp_f16_gguf): os.remove(temp_f16_gguf)\n",
    "        print(f\"üéä ¬°√âXITO TOTAL! Descarga: {output_final_gguf}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cr√≠tico: {e}\")\n",
    "\n",
    "# --- EJECUCI√ìN ---\n",
    "convert_and_quantize_ultimate_safe(\"/workspace/code/MiPM_Senior_HF\", \"/workspace/MiPM_Senior_Final.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03cb66-929a-4f88-8fd0-46b07915f3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry - Product Manager Senior)",
   "language": "python",
   "name": "mi-pm-senior-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
